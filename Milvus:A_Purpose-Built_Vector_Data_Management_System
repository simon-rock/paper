Milvus:A Purpose-Built Vector Data Management System

ABSTRACTRecently, there has been a pressing need to manage high-dimensionalvector data in data science and AI applications. This trend is fueledbythe proliferation of unstructured data and machine learning(ML),where ML models usually transform unstructured data intofeature vectors for data analytics, e.g., product recommendation.Existing systems and algorithms for managing vector data havetwo limitations: (1) They incur serious performance issue whenhandling large-scale and dynamic vector data; and (2) They pro-videlimited functionalities that cannot meet the requirements ofversatile applications.Thispaper presents Milvus, a purpose-built data managementsystemto e￿ciently manage large-scale vector data. Milvus sup-ports easy-to-use application interfaces (including SDKs and REST-fulAPIs); optimizes for the heterogeneous computing platform withmodern CPUs and GPUs; enables advanced query processing be-yond simple vector similarity search; handles dynamic data for fastupdates while ensuring e￿cient query processing; and distributesdataacross multiple nodes to achieve scalability and availability.We￿rst describe the design and implementation of Milvus. Thenwe demonstrate the real-world use cases supported by Milvus. Inparticular, we build a series of10applications(e.g., image/videosearch, chemical structure analysis, COVID-19 dataset search, per-sonalized recommendation, biological multi-factor authentication,intelligentquestion answering) on top of Milvus. Finally, we exper-imentallyevaluate Milvus with a wide range of systems includingtwo open source systems (Vearch and Microsoft SPTAG) and threecommercial systems. Experiments show that Milvus is up to twoorders of magnitude faster than the competitors while providingmore functionalities. Now Milvus is deployed by hundreds of orga-nizationsworldwide and it is also recognized as an incubation-stageproject of the LF AI & Data Foundation. Milvus is open-sourced athttps://github.com/milvus-io/milvus.


KEYWORDSVector database; High-dimensional similarity search; Heteroge-neous computing; Data science; Machine learningACM Reference Format:JianguoWang, Xiaomeng Yi, Rentong Guo, Hai Jin, Peng Xu, Shengjun Li,XiangyuWang, Xiangzhou Guo, Chengming Li, Xiaohai Xu, Kun Yu, Yux-ingYuan, Yinghao Zou, Jiquan Long, Yudong Cai, Zhenxiang Li, Zhifeng Zhang,Yihua Mo, Jun Gu, Ruiyi Jiang, Yi Wei, Charles Xie. 2021. Milvus: A Purpose-BuiltVector Data Management System. InProceedings of the 2021 Inter-national Conference on Management of Data (SIGMOD ’21), June 20–25,2021, Virtual Event, China.ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3448016.34575501  INTRODUCTIONAtZilliz, we have experienced a growing need from various cus-tomersto manage large-scale high-dimensional vector data (rang-ingfrom 10s to 1000s of dimensions) in many data science andAIapplications. This is largely due to two trends. The￿rst one isan explosive growth of unstructured data such as images, videos,texts, medical data, and housing data due to the prevalence ofsmartphones,IoT devices, and social media apps. According to IDC,80%of data will be unstructured by 2025 [36].The second trendisthe rapid development of machine learning that can e￿ectivelytransformunstructured data into learned feature vectors for dataanalytics.In particular, a recent popular approach in recommendersystemsis calledvector embeddingthat converts an item to a fea-ture vector (such as item2vec [11],word2vec [52],doc2vec [37],graph2vec [26])and provides recommendations via￿nding similarvectors [13,15,25,51].For example, YouTube embeds videos tovectors [15];Airbnb models houses with vectors [25];Bioscientistsdescribe the molecular structural information of drug compoundsusingvectors [13,51].Besides that, images and texts are also natu-rallyrepresented by vectors [8, 53].Thoseapplications present unique requirements and challengesfordesigning a scalable vector data management system. Theseinclude: (1) The need to support not only fast query processingon large-scale vector data but also the e￿cient handling of dy-namicvector data (such as insertions and deletions). As an example,Youtube uploads 500 hours of user-generated videos per minute andmeanwhileo￿ers real-time recommendations [67].(2) The need toprovide advanced query processing such as attribute￿ltering [65]andmulti-vector query processing [10]beyond simple vector simi-laritysearch. Here attribute￿ltering is to only search vectors thatsatisfya given￿ltering condition, which is useful in e-commerceapplications[65],e.g.,￿nding the T-shirts similar to a given imagevector that also cost less than $100. And multi-vector query pro-cessingtargets for the scenario where each object is described bymultiplevectors, e.g., pro￿ling a person using a face vector and aposture vector in many computer vision applications [10, 56].


Existing works on vector data management mainly focus on vec-tor similarity search [14,20,22,33,35,39,45,46,48,49,57,65,68],but they cannot meet the above requirements due to poor per-formance (on large-scale and dynamic vector data) and limitedfunctionalities (e.g., not being capable of supporting attribute ￿lter-ing and multi-vector queries) to support versatile data science andAI applications.More speci￿cally, we classify existing works into two categories:algorithmsandsystems. For the algorithmic works on vector simi-larity search, e.g., [20,22,33,45,46,48,49,57], together with theiropen-source implementation libraries (exempli￿ed by FacebookFaiss [35] and Microsoft SPTAG [14]), there are several limitations.(1) They are algorithms and libraries, not a full-￿edged system thatmanages vector data. They cannot handle large amount of datavery well since they assume that all the data and index are stored inmain memory and cannot span multiple machines. (2) Those worksusually assume data to be static once ingested into the system andcannot easily handle dynamic data while ensuring fast real-timesearches. (3) They do not support advanced query processing. (4)Those works are not optimized for the heterogeneous computingarchitecture with CPUs and GPUs.For the system works on vector similarity search, e.g., AlibabaAnalyticDB-V [65] and Alibaba PASE (PostgreSQL) [68], they fol-low the one-size-￿ts-all approach to extend relational databases forsupporting vector data by adding a table column called “vector col-umn” to store vectors. However, those systems are not specializedfor managing vector data and they do not treat vectors as ￿rst-classcitizens. (1) Legacy database components such as optimizer andstorage engine prevent ￿ne-tuned optimizations for vectors, e.g.,the query optimizer misses signi￿cant opportunity to best leverageCPU and GPU for processing vector data. (2) They do not supportadvanced query processing such as multi-vector queries.Another relevant system is Vearch [4,39], which is designedfor vector search. But Vearch is not e￿cient on large-scale data.Experiments (Figure 8 and Figure 15) show that Milvus, the systemintroduced in this paper, is 6.4⇥⇠47.0⇥faster than Vearch. Also,Vearch does not support multi-vector query processing.This paper presents Milvus, a purpose-built data managementsystem to e￿ciently store and search large-scale vector data fordata science and AI applications. It is a specialized system forhigh-dimensional vectors following the design practice of one-size-not-￿ts-all [60] in contrast to generalizing relational databases tosupport vectors. Milvus provides many application interfaces (in-cluding SDKs in Python/Java/Go/C++ and RESTful APIs) that canbe easily used by applications. Milvus is highly tuned for the het-erogeneous computing architecture with modern CPUs and GPUs(multiple GPU devices) for the best e￿ciency. It supports versatilequery types such as vector similarity search with various similarit functions, attribute ￿ltering, and multi-vector query processing.It provides di￿erent types of indexes (e.g., quantization-based in-dexes [33,35] and graph-based indexes [20,49]) and develops an ex-tensible interface to easily incorporate new indexes into the system.Milvus manages dynamic vector data (e.g., insertions and deletions)via an LSM-based structure while providing consistent real-timesearches with snapshot isolation. Milvus is also a distributed datamanagement system deployed across multiple nodes to achievescalability and availability. Table 1 highlights the main di￿erencesbetween Milvus and other systems.In terms of implementation, Milvus is built on top of FacebookFaiss [3,35], an open-source C++ library for vector similarity search.But Milvus signi￿cantly enhances Faiss with improved performance(e.g., optimizing for the heterogeneous computing platform in Sec. 3,supporting dynamic data management e￿ciently in Sec. 2.3 anddistributed query processing in Sec. 5.3), enhanced functionalities(e.g., attribute ￿ltering and multi-vector query processing in Sec. 4),and better usability (e.g., application interfaces in Sec. 2.1) to be afull-￿edged easy-to-use vector data management system.Product impact.Milvus is adopted by hundreds of organiza-tions and institutions worldwide in various ￿elds such as imageprocessing, computer vision, natural language processing, voicerecognition, recommender systems, and drug discovery. More im-portantly, Milvus was accepted as an incubation-stage project ofthe LF AI & Data Foundation in January 2020.1Contributions.This paper makes the following contributions:•System design and implementation(Sec. 2 and Sec. 5):The overall contribution is the design and implementationof Milvus, a purpose-built vector data management systemfor managing large-scale and dynamic vector data to enabledata science and AI applications. Milvus is open-sourced athttps://github.com/milvus-io/milvus.•Heterogeneous computing(Sec. 3): We optimize Milvusfor the heterogeneous hardware platform with modern CPUsand GPUs for fast query processing. For CPU-oriented de-sign, we propose both cache-aware and SIMD-aware (e.g.,SSE, AVX, AVX2, AVX512) optimizations. For GPU-orienteddesign, we design a new hybrid index that takes advantagesof the best of CPU and GPU, and we also develop a newscheduling strategy to support multiple GPU devices.•Advanced query processing(Sec. 4): We support attribute￿ltering and multi-vector query processing beyond simplevector similarity search in Milvus. In particular, we designa new partition-based algorithm for attribute ￿ltering andtwo algorithms (vector fusion and iterative merging) formulti-vector query processing.
•Novel applications(Sec. 6): We describe novel applicationspowered by Milvus. In particular, we build a series of10ap-plications2on top of Milvus to demonstrate its broad appli-cability including image search, video search, chemical struc-ture analysis, COVID-19 dataset search, personalized recom-mendation, biological multi-factor authentication, intelligentquestion answering, image-text retrieval, cross-modal pedes-trian search, and recipe-food search.

2SYSTEM DESIGNIn this section, we present an overview of Milvus. Figure 1 showsthe architecture of Milvus with three major components: queryengine, GPU engine, and storage engine. The query engine sup-ports e￿cient query processing over vector data and it is optimizedfor modern CPUs by reducing cache misses and leveraging SIMDinstructions. The GPU engine is a co-processing engine that accel-erates performance with vast parallelism. It also supports multipleGPU devices for e￿ciency. The storage engine enables data dura-bility and incorporates an LSM-based structure for dynamic datamanagement. It runs on various ￿le systems (including local ￿lesystems, Amazon S3, and HDFS) with a bu￿erpool in memory.


2.1Query ProcessingWe ￿rst present the concept of entity used in Milvus and thenexplain query types, similarity functions, and application interfaces.Entity.To best capture versatile data science and AI applica-tions, Milvus supports query processing over both vector data andnon-vector data. We de￿ne the termentityas follows to incorporatethe two. Each entity in Milvus is described as one or more vectorsand optionally some numerical attributes (non-vector data). Forexample, in the image search application, the numerical attributescan represent the age and height of a person in addition to possiblymultiple machine-learned feature vectors of his/her photos (e.g., de-scribing front-face, side-face, or posture [10]). In the current versionof Milvus, we only support numerical attributes as observed frommany applications. But in the future, we plan to support categoricalattributes with indexes like inverted lists or bitmaps [64].

Query types.Milvus supports three primitive query types:•Vector query: This query type is the traditional vector simi-larity search [33,41,48,49], where each entity is describedas a single vector. The system returns:most similar vectorswhere:is a user-input parameter.•Attribute ￿ltering: Each entity is speci￿ed by a single vectorand some attributes [65]. The system returns:most similarvectors while adhering to the attributes constraints. As anexample in recommender systems, users want to ￿nd similarclothes to a given query image while the price is below $100.•Multi-vector query: Each entity is stored as multiple vec-tors [10]. The query returns top-:similar entities accordingto an aggregation function (e.g., weighted sum) betweenmultiple vectors.

Similarity functions.Milvus o￿ers commonly used similaritymetrics, including Euclidean distance, inner product, cosine similar-ity, Hamming distance, and Jaccard distance, allowing applicationsto explore vector similarity in the most e￿ective approach.

Application interfaces.Milvus provides easy-to-use SDK (soft-ware development kit) interfaces that can be directly called in ap-plications written in various languages including Python, Java, Go,and C++. Milvus also supports RESTful APIs for web applications.

2.2IndexingIndexing is of tremendous importance to query processing in Milvus.But a challenging issue we face is to decide which indexes to supportin Milvus, because there are numerous indexes developed for vectorsimilarity search. The latest benchmark [41] shows that there isno winner in all scenarios and each index comes with tradeo￿s inperformance, accuracy, and space overhead.In Milvus, we mainly support two types of indexes:3quantization-based indexes (including IVF_FLAT [3,33,35], IVF_SQ8 [3,35],and IVF_PQ [3,22,33,35]) and graph-based indexes (includingHNSW [49] and RNSG [20]) to serve di￿erent applications. Thedesign decision is based on factors including the latest literaturereview [41], industrial-strength systems (e.g., Alibaba PASE [68],Alibaba AnalyticDB-V [65], Jingdong Vearch [39]), open-source li-braries (e.g., Facebook Faiss [3,35]), and inputs from customers. Weexclude LSH-based approaches because they have lower accuracythan quantization-based approaches on billion-scale data [65, 68].Considering there are many new indexes coming out every year,Milvus is designed to easily incorporate the new indexes with ahigh-level abstraction. Developers only need to implement a fewpre-de￿ned interfaces for adding a new index. Our hope is thatMilvus can eventually become a standard platform for vector datamanagement with versatile indexes.

2.3Dynamic Data ManagementMilvus supports e￿cient insertions and deletions by adopting theidea of LSM-tree [47]. Newly inserted entities are stored in memory￿rst as MemTable. Once the accumulated size reaches a threshold,or once every second, the MemTable becomes immutable and thengets ￿ushed to disk as a new segment. Smaller segments are mergedinto larger ones for fast sequential access. Milvus implements atiered merge policy (also used in Apache Lucene) that aims tomerge segments of approximately equal sizes until a con￿gurablesize limit (e.g., 1GB) is reached. Deletions are supported in thesame out-of-place approach except that the obsoleted vectors are removed during segment merge. Updates are supported by deletionsand insertions. By default, Milvus builds indexes only for largesegments (e.g.,>1GB) but users are allowed to manually buildindexes for segments of any size if necessary. Both index and dataare stored in the same segment. Thus, the segment is the basic unitof searching, scheduling, and bu￿ering.Milvus o￿ers snapshot isolation to make sure reads and writesshare a consistent view and do not interfere with each other. Wepresent the details of snapshot isolation in Sec. 5.2.

2.4Storage ManagementAs mentioned in Sec. 2.1, each entity is expressed as one or morevectors and optionally some attributes. Thus, each entity can beregarded as a row in an entity table. To facilitate query processing,Milvus physically stores the entity table in a columnar fashion.Vector storage.For single-vector entities, Milvus stores all thevectors continuously without explicitly storing the row IDs. In thisway, all the vectors are sorted by row IDs. Given a row ID, Milvuscan directly access the corresponding vector since each vector is ofthe same length. For multi-vector entities, Milvus stores the vectorsof di￿erent entities in a columnar fashion. For example, assumingthat there are three entities (,⌫, and⇠) in the database and eachentity has two vectorsv1andv2, then all thev1of di￿erent entitiesare stored together and all thev2are stored together. That is, thestorage format is {.v1,⌫.v1,⇠.v1,.v2,⌫.v2,⇠.v2}.Attribute storage.The attributes are stored column by col-umn. In particular, each attribute column is stored as an array ofhkey,valueipairs where thekeyis the attribute value andvalueisthe row ID, sorted by thekey. Besides that, we build skip pointers(i.e., min/max values) following Snow￿ake [16] as indexing for thedata pages on disk. This allows e￿cient point query and rangequery in that column, e.g., price is less than $100.Bu￿erpool.Milvus assumes that most (if not all) data and indexare resident in memory for high performance. If not, it relies onan LRU-based bu￿er manager. In particular, the caching unit is asegment, which is the basic searching unit as explained in Sec. 2.3.Multi-storage.For ￿exibility and reliability, Milvus supportsmultiple ￿le systems including local ￿le systems, Amazon S3, andHDFS for the underlying data storage. This also facilitates the de-ployment of Milvus in the cloud.2.5Heterogeneous ComputingMilvus is highly optimized for the heterogeneous computing plat-form that includes CPUs and GPUs. Sec. 3 presents the details.2.6Distributed SystemMilvus can function as a distributed system deployed across multi-ple nodes. It adopts modern design practices in distributed systemsand cloud systems such as storage/compute separation, shared stor-age, read/write separation, and single-writer-multi-reader. Sec. 5.3explains more.3HETEROGENEOUS COMPUTINGIn this section, we present the optimizations for Milvus to bestleverage the heterogeneous computing platform involving bothCPUs and GPUs to achieve high performance.


As explained in Sec. 2.2, Milvus mainly supports quantization-based indexes (including IVF_FLAT [3,33,35], IVF_SQ8 [3,35],and IVF_PQ [3,22,33,35]) and graph-based indexes (includingHNSW [49] and RNSG [20]). In this section, we use quantization-based indexes to illustrate our optimizations because they consumemuch less memory and are much faster to build index while achiev-ing decent query performance when compared to graph-basedindexes [65,68]. Note that many optimizations (such as SIMD andGPU optimizations) can be applied to graph-based indexes.3.1BackgroundBefore diving into optimizations, we explain vector quantizationand quantization-based indexes. The main idea of vector quantiza-tion is to apply a quantizerIto map a vectorvto a codewordI(v)chosen from a codebookC[33]. TheK-means clustering algorithmis commonly used to construct the codebookCwhere each code-word is the centroid andI(v)is the closest centroid tov. Figure 2shows an example of 10 vectors (v0tov9) of three clusters withcentroids beingc0toc2, thenI(v0),I(v1),I(v2), orI(v3)isc0.Quantization-based indexes (such as IVF_FLAT [3,33,35], IVF_SQ8[3,35], and IVF_PQ [3,22,33,35]) use two quantizers: coarse quan-tizer and ￿ne quantizer. The coarse quantizer applies the -meansalgorithm (e.g., is 16384 in Milvus and Faiss [3]) to cluster vec-tors into buckets. And the ￿ne quantizer encodes the vectorswithin each bucket. Di￿erent indexes may use di￿erent ￿ne quan-tizers. IVF_FLAT uses the original vector representation; IVF_SQ8uses a compressed representation for the vectors by adopting one-dimensional quantizer (called “scalar quantizer”) to compress a4-byte ￿oat value to a 1-byte integer; and IVF_PQ uses productquantization that splits each vector into multiple sub-vectors andapplies -means for each sub-space.Query processing (of a queryq) over quantization-based indexestakes two steps: (1) Find the closest=?A>14buckets (or clusters)based on the distance betweenqand the centroid of each bucket.For example, assuming=?A>14is 2 in Figure 2, then the closesttwo buckets ofqare centered atc0andc1. The parameter=?A>14controls the tradeo￿between accuracy and performance. Higher=?A>14produces better accuracy but worse performance. (2) Searchwithin each of the=?A>14relevant buckets based on di￿erent ￿nequantizers. For example, if the index in Figure 2 is IVF_FLAT, thenit needs to scan the vectorsv0tov6in the two buckets.3.2CPU-oriented Optimizations3.2.1  Cache-aware Optimizations in MilvusThe fundamental problem for query processing over quantization-based indexes is that, given a collection of<queries {q1,q2, ...,q<}and a collection of=data vectors {v1,v2, ...,v=}, how to quickly ￿nd for each queryq8its top-:similar vectors? In practice, userscan submit batch queries so that<1.This operation happens in ￿nding the relevant buckets as well assearching within each relevant bucket. The original implementationin Facebook Faiss [3], which Milvus is built on top of, is ine￿cientbecause it incurs many CPU cache misses as explained below. Thus,Milvus develops an optimized approach to signi￿cantly reduce datamovement between main memory and CPU caches.Original implementation in Facebook Faiss [3].Faiss usesthe OpenMP multi-threading to process queries in parallel. Eachthread is assigned to work on a single query at a time. The threadis released (for next query) once the current task is ￿nished. Eachtask comparesq8with all the=data vectors and maintains a:-sizedheap to store the results.The above solution in Faiss has two performance issues: (1) Itincurs many CPU cache misses, because for each query the entiredata needs to be streamed through CPU caches and cannot bereused for the next query. Thus, each thread accesses</Ctimes ofthe entire data whereCis the total number of threads. (2) It cannotfully leverage multi-core parallelism when the batch size<is small.Optimizations in Milvus.Milvus develops two ideas to tacklethe issues. First, it reuses the accessed data vectors as much as possi-ble for multiple queries to minimize CPU cache misses. Speci￿cally,it optimizes for reducing L3 cache misses because the penalty toaccess memory is high and also L3 cache size (typically 10s MB)is much bigger than L1/L2 cache, leaving more room for optimiza-tions. Second, it uses ￿ne-grained parallelism that assigns threadsto data vectors instead of query vectors to best leverage multi-coreparallelism, because the data size=is usually much bigger than thequery size<in practice.Figure 3 shows the overall design. Speci￿cally, letCbe the num-ber of threads, then each thread)8is assigned1==/Cdata vec-tors:4{v(81)⇤1,v(81)⇤1+1,...,v8⇤11}. Milvus then partitions the<queries into query blocks of sizeBsuch that each query block(together with its associated heaps) can always ￿t in the L3 CPUcache. We decideBlater on in Equation (1). Here we assume that<is divisible byB. Milvus computes the top-:results of each queryblock at a time with multiple threads. Whenever each thread loadsits assigned data vectors to L3 cache, they will be compared againstthe entire query block (withBqueries) in the cache. To minimizethe synchronization overhead, Milvus assigns a heap per queryper thread. In particular, assuming the8-th query block{q(81)⇤B,q(81)⇤B+1, ...,q8⇤B1}is in cache, Milvus dedicates the heapA1,91for the9-th queryq(81)⇤B+91on theA-th thread)A1. Thus, theresults of a queryq8are spread overCthreads of heaps. Thus, itneeds to merge the heaps of each thread to obtain the ￿nal top-:results.Next, we discuss how to determine the query block sizeBsuchthatBqueries and their associated heaps can always ￿t in L3 cache.Let3be the dimensionality, then the size of each query is3⇥sizeof(￿oat). Since each heap entry contains a pair of vector ID andsimilarity, then the total size of the heaps (per query) isC⇥:⇥(sizeof(int64) + sizeof(￿oat)) whereCis the number of threads. Thus,Bis computed as follows:B=L3’s cache size3⇥sizeof(￿oat)+C⇥:⇥(sizeof(int64)+sizeof(￿oat)).(1)

In this way, each thread only accesses</(B⇤C)times of the entiredata, which isBtimes smaller than the original implementation inFacebook Faiss [3]. Experiments (Sec. 7.4) show that this improvesperformance by a factor of 1.5⇥to 2.7⇥.3.2.2  SIMD-aware Optimizations in MilvusModern CPUs support increasingly wider SIMD instructions.Thus, it is not surprising that Facebook Faiss [3] implements SIMD-aware algorithms to accelerate vector similarity search. We maketwo engineering optimizations in Milvus: (1) Supporting AVX512;and (2) Automatic SIMD-instruction selection.Supporting AVX512.Faiss [3] does not support AVX512, whichis now available in mainstream CPUs. Thus, we extend the similaritycomputing function with AVX512 instructions, such as_mm512_add_ps,_mm512_mul_ps, and_mm512_extractf32x8_ps. Now Milvus sup-ports SIMD SSE, AVX, AVX2, and AVX512.Automatic SIMD-instruction selection.Milvus is designedto work well on a wide spectrum of CPU processors (both on-premises and cloud platforms) with di￿erent SIMD instructions(e.g., SIMD SSE, AVX, AVX2, and AVX512). Thus the challenge is,given a single piece of software binary (i.e., Milvus), how to makeit automatically invoke the suitable SIMD instructions on any CPUprocessor? Faiss [3] does not support it and users need to manuallyspecify the SIMD ￿ag (e.g., “-msse4”) during compilation time. InMilvus, we take a considerable amount of engineering e￿ort torefactor the codebase of Faiss. We factor out the common functions(e.g., similarity computing) that rely on SIMD accelerations. Thenfor each function, we implement four versions (i.e., SSE, AVX, AVX2,AVX512) and put each one into a separated source ￿le, which isfurther compiled individually with the corresponding SIMD ￿ag.During runtime, Milvus can automatically choose the suitable SIMDinstructions based on the current CPU ￿ags and then link the rightfunction pointers using hooking.

3.3GPU-oriented OptimizationsGPU is known for vast parallelism and Faiss [3] supports GPU forquery processing over vector data. Milvus enhances Faiss in twoaspects: (1) Supporting bigger:in the GPU kernel; (2) Supportingmulti-GPU devices.Supporting bigger:in GPU kernel.The original implemen-tation in Faiss [3] does not support top-:query processing where:is greater than 1024 due to the limit of shared memory. But many application such as video surveillance and recommender systemsmay need bigger:for further veri￿cation or re-ranking [69, 71].Milvus overcomes this limitation and supports:up to 16384although technically Milvus can support any:.5When:is largerthan 1024, Milvus executes the query in multiple rounds to cumu-latively produce the ￿nal results. In the ￿rst round, Milvus behavesthe same as Faiss and gets the top 1024 results. For the secondand later rounds, Milvus ￿rst checks the distance of the last re-sult (denoted as3;) in the previous round. Apparently,3;is so farthe largest distance in the partial results.To handle vectors withequivalent distance to the query, Milvus also records vector IDs inthe result whose distances are equal to3;. Then Milvus ￿lters outvectors whose distances are smaller than3;or IDs are recorded.From the remaining data, Milvus gets the next 1024 results.Bydoing so, Milvus ensures that results in previous rounds will notappear in the current round. After that, the new results are mergedwith the partial results obtained in earlier rounds. Milvus processesthe query in a round-by-round fashion until a su￿cient number ofresults are collected.Supporting multi-GPU devices.Faiss [3] supports multipleGPU devices since they are usually found in modern servers. ButFaiss needs to declare all the GPU devices in advance during com-pilation time. That means if the Faiss codebase is compiled using aserver with2GPUs, then the software binary can only be runningin a server that has at least2GPUs.Milvus overcomes this limitation by allowing users to select anynumber of GPU devices duringruntime(instead of compilationtime). As a result, once the Milvus codebase is compiled into asoftware binary, it can run at any server. Under the hood, Milvusintroduces a segment-based scheduling that assigns segment-basedsearch tasks to the available GPU devices. Each segment can onlybe served by a single GPU device. This is particularly a good ￿t forthe cloud environment with dynamic resource management whereGPU devices can be elastically added or removed. For example,if there is a new GPU device installed, Milvus can immediatelydiscover it and assign the next available search task to it.3.4GPU and CPU Co-designIn this mode, the GPU memory is not large enough to store theentire data. Facebook Faiss [3] alleviates the problem by using alow-footprint compressed index (called IVF_SQ8 [3])6and mov-ing data from CPU memory to GPU memory (via PCIe bus) ondemand. However, we ￿nd that there are two limitations: (1) ThePCIe bandwidth is not fully utilized, e.g., our experiments showthat the measured I/O bandwidth is only 1⇠2GB/s while PCIe 3.0(16x) supports up to 15.75GB/s. (2) It is not always bene￿cial toexecute queries on GPU (than CPU) considering the data transfer.Milvus develops a new index called SQ8H (where ‘H’ stands forhybrid) to address the above limitations (Algorithm 1).Addressing the ￿rst limitation.We investigate the codebaseof Faiss and ￿gure out that Faiss copies data (from CPU to GPU)bucket by bucket, which underutilizes the PCIe bandwidth sinceeach bucket can be small. So the natural idea is to copy multiple buckets simultaneously. But the downside of such multi-bucket-copying is the handling of deletions where Faiss uses a simple in-place update approach because each bucket is copied (and stored)individually. Fortunately, deletions (and updates) are easily handledin Milvus since Milvus adopts an e￿cient LSM-based out-of-placeapproach (Sec. 2.3).As a result, Milvus improves the I/O utilizationby copying multiple buckets if possible (line 3 of Algorithm 1).Addressing the second limitation.We observe that GPU out-performs CPU only if the query batch size is large enough consid-ering the expensive data movement. That is because more queriesmake the workload more computation-intensive since they searchthe same data. Thus, if the batch size is bigger than a threshold(e.g., 1000), Milvus executes all the queries in GPU and loads neces-sary buckets if GPU memory is insu￿cient (line 2 of Algorithm 1).Otherwise, Milvus executes the query in a hybrid manner as fol-lows. As mentioned in Sec. 3.1, there are two steps for searchingquantization-based indexes: ￿nding=?A>14relevant (closest) buck-ets and scanning each relevant bucket. Milvus executes step 1 inGPU and step 2 in CPU because we observe that step 1 has a muchhigher computation-to-I/O ratio than step 2 (line 5 and 6 in Algo-rithm 1). That is because in step 1, all the queries compare againstthesame centroids to ￿nd=?A>14nearest buckets, and also the centroids are small enough to be resident in the GPU memory. Bycontrast, data accesses in step 2 are more scattered since di￿erentqueries do not necessarily access the same buckets.4ADVANCED QUERY PROCESSING4.1Attribute FilteringAs mentioned in Sec. 2.1, attribute ￿ltering is a hybrid query typethat involves both vector data and non-vector data [65]. It onlysearches vectors that satisfy the attributes constraints. It is crucialto many applications [65], e.g., ￿nding similar houses (vector data)whose sizes are within a speci￿c range (non-vector data). For pre-sentation purpose, we assume that each entity is associated witha single vector and a single attribute since it is straightforward toextend the algorithms to multiple attributes. We defer multi-vectorquery processing to Sec. 4.2.Formally, each such query involves two conditionsCandC+whereCspeci￿es the attribute constraint andC+is the normalvector query constraint that returns top-:similar vectors. Withoutloss of generality,Cis represented in the form of0>=?1&&0<=?2where0is the attribute (e.g., size, price) and?1and?2aretwo boundaries of a range condition (e.g.,?1=100and?2=500).There are several approaches to solve attribute ￿ltering as re-cently studied in AnalyticDB-V [65]. In Milvus, we implement those approaches (i.e., strategies A, B, C, D as explained below). We thenpropose a partition-based approach (i.e., strategy E), which is upto 13.7⇥faster than the strategy D (i.e., state-of-the-art solution)according to the experiments in Sec. 7.5. Figure 4 shows a summaryand we present the details next.Strategy A: attribute-￿rst-vector-full-scan.It only uses theattribute constraintCto obtain relevant entities via index search.Since the data is stored mostly in memory, we use binary search,but a B-tree index is also possible. When data cannot ￿t in memory,we use skip pointers for fast search.After that, all the entitiesin the result set are fully scanned to compare against the queryvector to produce the ￿nal top-:results. Although simple, thisapproach is suitable whenCis highly selective such that only asmall number of candidates are required for further veri￿cation.Another interesting property of this strategy is that it produces theexact results.Strategy B: attribute-￿rst-vector-search.The di￿erence withthe strategy A is that after it obtains the relevant entities accordingto attribute constraintC, it produces a bitmap of the resultant en-tity IDs. Then it conducts the normal vector query processing basedonC+and checks the bitmap whenever a vector is encountered.Only vectors that pass bitmap testing are included in the ￿nal top-:results. This strategy is suitable in many cases whenCorC+ismoderately selective.Strategy C: vector-￿rst-attribute-full-scan.In contrast tothe strategy A, this approach only uses the vector constraintC+to obtain the relevant entities via vector indexing like IVF_FLAT.Then the resultant entities are fully scanned to verify if they satisfythe attribute constraintC. To make sure there are:￿nal results, itsearches for\·:(\>1) results during the vector query processing.This strategy is suitable when the vector constraintC+is highlyselective that the number of candidates is relatively small.Strategy D: cost-based.It is a cost-based approach that esti-mates the cost of the strategy A, B, C, and picks up the one withthe least cost as proposed in AnalyticDB-V [65]. From [65] and ourexperiments, the cost-based strategy is suitable in almost all cases.Strategy E: partition-based.This is a partition-based approachthat we develop in Milvus. The main idea is that it partitions thedataset based on the frequently searched attribute and applies thecost-based approach (i.e., the strategy D) for each partition.Inparticular, we maintain the frequency of each searched attribute in a hash table and increase the counter whenever a query refers tothat attribute.Given a query of attribute ￿ltering, it only searchesthe partitions whose attribute-ranges overlap with the query range.More importantly, if the range of a speci￿c partition is coveredby the query range, then this strategy does not need to check theattribute constraint (C) anymore and only focuses on vector queryprocessing (C+) in that partition, because all the vectors in thatpartition satisfy the attribute constraint.As an example, suppose that there are many queries involvingthe attribute ‘price’ and the strategy E splits the dataset into ￿vepartitions:P0[1⇠100],P1[101⇠200],P2[201⇠300],P3[301⇠400],P4[401⇠500]. Then if the attribute constraint (C) of the query is[50⇠250], then onlyP0,P1, andP2are necessary for searchingbecause their ranges overlap with the query range. And whensearchingP1, there is no need to check the attribute constraintsince its range is completely covered by the query range. This cansigni￿cantly improve the query performance.In the current version of Milvus, we create the partitions o￿inebased on historical data and serve query processing online. Thenumber of partitions (denoted asd) is a parameter con￿gured byusers. Choosing a properdis subtle: Ifdis too small, then eachpartition contains too many vectors and it becomes hard to pruneirrelevant partitions for this strategy; Ifdis too big, then the numberof vectors in each partition is so small that the vector indexingdeteriorates towards linear search. Based on our experience, werecommenddto be chosen such that each partition contains roughly1 million vectors. For example, on a billion-scale dataset, there arearound 1000 partitions. However, it is an interesting future work toinvestigate the use of machine learning and statistics to dynamicallypartition the data and decide the right number of partitions.4.2Multi-vector QueriesIn many applications, each entity is speci￿ed by multiple vectors foraccuracy. For example, intelligent video surveillance applicationsuse di￿erent vectors to describe the front face, side face, and posturefor each person captured on camera [10]. Recipe search applicationsuse multiple vectors to represent text description and associatedimages for each recipe [56]. Another source of multi-vector is thatmany applications use more than one machine learning model evenfor the same object to best describe that object [30, 69].Formally, each entity contains`vectorsv0,v1, ...,v`1. Then amulti-vector query ￿nds top-:entities according to an aggregatedscoring function6over the similarity function5(e.g., inner product)of each individual vectorv8. Speci￿cally, the similarity of two enti-ties-and.is computed as6(5(-.v0,..v0),...,5(-.v`1,..v`1))where-.v8means the vectorv8of the entity-. To capture a widerange of applications, we assume the aggregation function6to bemonotonic in the sense that6is non-decreasing with respect toevery5(-.v8,..v8)[19]. In practice, many commonly used aggrega-tion functions are monotonic, e.g., weighted sum, average/median,and min/max.Naive solution.LetDbe the dataset andD8is a collection ofv8of all the entities, i.e.,D8={4.v8|42D}. Given a query@, the naivesolution is to issue an individual top-:query for each vector@.v8onD8to produce a set of candidates, which are further computedto obtain the ￿nal top-:results. Although simple, it can miss manytrue results leading to extremely low recall (e.g., 0.1).This approach was widely used in the area of AI and machine learning to supporte￿ective recommendations, e.g., [29, 70].In Milvus, we develop two new approaches, namely vector fusionand interactive merging that target for di￿erent scenarios.Vector fusion.We illustrate the vector fusion approach assum-ing that the similarity function is inner product and we will explainhow to extend to other similarity functions afterwards. Let4be anarbitrary entity in the dataset andv0,v1, ...,v`1be the`vectorsthat each entity contains, this approach stores for each entity4its`vectors as a concatenated vectorv=[4.v0,4.v1,...,4.v`1]. Let@be a query entity, during query processing, this approach appliesthe aggregation function6to the`vectors of@, producing an ag-gregated query vector. For example, if the aggregation function isweighted sum withF8for each weight, then the aggregated queryvector is:[F0⇥@.v0,F1⇥@.v1,...,F`1⇥@.v`1]. Then it searchesthe aggregated query vector against the concatenated vectors inthe dataset to obtain the ￿nal results. It is straightforward to provethe correctness of vector fusion because the similarity function ofinner product is decomposable.The vector fusion approach is simple and e￿cient because it onlyneeds to invoke the vector query processing once. But it requiresa decomposable similarity function such as inner product. Thissounds restrictive but when the underlying data is normalized,many similarity functions such as cosine similarity and Euclideandistance can be converted to inner product equivalently.Iterative merging.If the underlying data is not normalizedandthe similarity function is not decomposable (e.g., Euclideandistance), then the above vector fusion approach is not applica-ble. Then we develop another algorithm called iterative merging(see Algorithm 2) that is built on top of Fagin’s well-known NRAalgorithm [19], a general technique for top-:query processing.7Our initial try is actually to use the NRA algorithm [19] bytreating the results of each@.v8onD8as a stream provided byMilvus. However, we quickly ￿nd that it is ine￿cient because NRAfrequently callsgetNext()to obtain the next result of@.v8in-teractively. However, existing vector indexing techniques such asquantization-based indexes and graph-based indexes do not supportgetNext()e￿ciently. A full search is required to get the next result.Another drawback of NRA is that, it incurs signi￿cant overhead tomaintain the heap since every access in NRA needs to update thescores of the current objects in the heap.Thus, iterative merging makes two optimizations over NRA: (1)It does not rely ongetNext()and instead callsVector￿ery(@.v8,D8,:0) with adaptive:0to get the top-:0query results of@.v8.Asa result, it does not need to invoke the vector query processingfor every access as NRA does. It can also eliminate the expensiveoverhead of the heap maintenance as in NRA. (2) It introduces anupper bound of the maximum number of steps to access since thequery results in Milvus are approximate.Algorithm 2 shows iterative merging. The main idea is that ititeratively issues a top-:0query processing for each@.v8onD8and puts the results toR8, whereD8is a collection ofv8of all theentities in the datasetD, i.e.,D8={4.v8|42D}, see line 3 and 4in Algorithm 2. Then it executes the NRA algorithm over all theR8.If at least:results can be fully determined (line 5), i.e., NRA cansafely stop, then the algorithm can terminate since top-:results can be produced.Otherwise, it doubles:0and iterates the processuntil:0reaches to a pre-de￿ned threshold (line 2).In contrast to the vector fusion approach, the iterative mergingapproach makes no assumption on the data and similarity func-tions, thus it can be used in a wide spectrum of scenarios. But theperformance will be worse than vector fusion when the similarityfunction is decomposable.Note that in the database ￿eld, there are many top-:algorithmsproposed, e.g., [5,12,31,42,62]. However, those algorithms can-not be directly used to solve the multi-vector query processing,because the underlying vector indexes cannot supportgetNext()e￿ciently as mentioned earlier. The proposed iterative mergingapproach (Algorithm 2) is a generic framework so that it is possibleto incorporate other top-:algorithms (e.g., [42]) by replacing line 5.But it remains an open question in terms of optimality and it is alsointeresting to optimize multi-vector query processing in the future.5SYSTEM IMPLEMENTATIONIn this section, we present the implementation details of asynchro-nous processing, snapshot isolation, and distributed computing.5.1Asynchronous ProcessingMilvus is designed to minimize the foreground processing via asyn-chronous processing to improve throughput. When Milvus receivesheavy write requests, it ￿rst materializes the operations (similar todatabase logs) to disk and then acknowledges to users. There is abackground thread that consumes the operations. As a result, usersmay not immediately see the inserted data. To prevent this, Milvusprovides an APIflush()that blocks all the incoming requests untilthe system ￿nishes processing all the pending operations. Besidesthat, Milvus builds indexes asynchronously.5.2Snapshot IsolationMilvus provides snapshot isolation to make sure reads and writessee a consistent view since Milvus supports dynamic data manage-ment. Every query only works on the snapshot when the querystarts. Subsequent updates to the system will create new snapshotsand do not interfere with the on-going queries.Milvus manages dynamic data following the LSM-style. All thenew data are inserted to memory ￿rst and then ￿ushed to disk asimmutable segments. Each segment has multiple versions and a new version is generated whenever the data or index in that segmentis changed (e.g., upon ￿ushing, merging, or building index). Allthe latest segments at any time form a snapshot. Each segmentcan be referenced by one or more snapshots.When the systemstarts, there are no segments. Assuming that there are some inserts￿ushed to disk atC1, which forms segment 1. Later on atC2, segment2 is generated. Now there are two snapshots in the system wheresnapshot 1 points to segment 1 and snapshot 2 points to bothsegment 1 and segment 2. So the segment 1 is referenced by twosnapshots. All the queries beforeC2work on snapshot 1 and all thequeries afterC2work on snapshot 2. There is a background threadto garbage collect the obsolete segments if they are not referenced.Note that the snapshot isolation is applied to the internal datareorganizations in the LSM structure. In this way, all the (internal)reads are not blocked by writes.

5.3Distributed SystemFor scalability and availability, Milvus is a distributed system thatsupports data management across multiple nodes. From the highlevel, Milvus is a shared-storage distributed system that separatescomputing from storage to achieve the best elasticity. The shared-storage architecture is widely used in modern cloud systems suchas Snow￿ake [16] and Aurora [63].Figure 5 shows the overall architecture consisting of three layers.The storage layer is based on Amazon S3 (also used in Snow￿ake [16])because S3 is highly available. The computing layer processes userrequests such as data insertions and queries. It also has local mem-ory and SSDs for caching data to minimize frequent accesses to S3.Besides that, there is a coordinator layer to maintain the metadataof the system such as sharding and load balancing information. Thecoordinator layer is highly available with three instances managedby Zookeeper.Next, we elaborate more on the computing layer, which is state-less to achieve elasticity. It includes a single writer instance andmultiple reader instances since Milvus is read-heavy and currentlya single writer is su￿cient to meet the customer needs. The writerinstance handles data insertions, deletions, and updates. The readerinstances process user queries. Data is sharded among the readerinstances with consistent hashing. The sharding information isstored in the coordinator layer. There are no cross-shard transac-tions since there are no mixed reads and writes in the same request.The design achieves near-linear scalability as shown in the exper-iments (Figure 10). All the computing instances are managed byKubernetes (K8s). When an instance is crashed, K8s will automat-ically restart a new instance to replace the old one. If the writerinstance crashes, Milvus relies on WAL (write-ahead logging) toguarantee atomicity. Since the instances are stateless, crashing will not a￿ect data consistency. Besides that, K8s can also elasticallyadd more reader instances if existing ones are overloaded.To minimize the network overhead between computing andstorage, Milvus employs two optimizations: (1) The computinglayer only sends logs (rather than the actual data) to the storagelayer, similar to Aurora [63]. As mentioned in Sec. 5.1, Milvus asyn-chronously processes the logs with a background thread to improveperformance. In the current implementation, the background threadcomes from the writer instance since the writer’s load is not toohigh. Otherwise, the log processing can be managed by a dedicatedinstance. (2) Another optimization is that each computing instancehas a signi￿cant amount of bu￿er memory and SSDs to reduceaccesses to the shared storage.6APPLICATIONSIn this section, we present applications that are powered by Milvus.We have built10 applicationson top of Milvus that includes imagesearch, video search, chemical structure analysis, COVID-19 datasetsearch, personalized recommendation, biological multi-factor au-thentication, intelligent question answering, image-text retrieval,cross-modal pedestrian search, and recipe-food search. This sectionpresents two of them due to space limit and more can be found inhttps://github.com/milvus-io/bootcamp/tree/master/EN_solutions.6.1Image SearchImage search is a well known application of vector search whereeach image is naturally converted to a vector using deep learningmodels such as VGG [58] and ResNet [28].Two tech companies, Qichacha8and Beike Zhaofang,9currentlyuse Milvus for large-scale image searches. Qichacha is a leadingChinese website for storing and searching business information (ofover 100 million companies), e.g., the names of o￿cers/shareholdersand credit information. Milvus supports Qichacha in ￿nding similartrademarks for customers to check if their trademarks have beenregistered. Beike Zhaofang is one of the biggest online real estatetransaction platform in China. Milvus supports Beike Zhaofang in￿nding similar houses and apartments (e.g., ￿oor plans). Figure 6shows an example of searching business trademarks and houses inQichacha and Beike Zhaofang using Milvus.

6.2Chemical Structure AnalysisChemical structure analysis is an emerging application that dependson vector search. Recent studies have demonstrated that a newe￿cient paradigm of understanding the structure of a chemicalsubstance is to encode it into a high-dimensional vector and usevector similarity search (e.g., with Tanimoto distance [9]) to ￿ndsimilar structures [9, 66].Milvus is now adopted by Apptech,10a major pharmaceuticalcompany developing new medicines and medical devices. Milvussigni￿cantly reduces the time of chemical structure analysis fromhours to less than a minute. Figure 7 shows an example of searchingsimilar chemical structures using Milvus.7EXPERIMENTS7.1Experimental SetupExperimental platform.We conduct all the experiments on Al-ibaba Cloud and use di￿erent types of computing instances (upto 12 nodes) for di￿erent experiments to save monetary cost. Bydefault, we use the CPU instance of ecs.g6e.4xlarge (Xeon Platinum8269 Cascade 2.5GHz, 16 vCPUs, 35.75MB L3 cache, AVX512, 64GBmemory, and NAS elastic storage). The GPU instance is ecs.gn6i-c16g1.4xlarge (NVIDIA Tesla T4, 64KB private memory, 512KB localmemory, 16GB global memory, and PCIe 3.0 16x interface).Datasets.To be reproducible, we use the following two publicdatasets to evaluate Milvus: SIFT1B [34] and Deep1B [8]. SIFT1Bcontains 1 billion 128-dimensional SIFT vectors (512GB) and Deep1Bcontains 1 billion 96-dimensional image vectors (384GB) extractedfrom a deep neural network. Both are standard datasets used inmany previous works on vector similarity search and approximatenearest neighbor search [35, 41, 65, 68].Competitors.We compare Milvus against two open-source sys-tems: Jingdong Vearch (v3.2.0) [4,39] and Microsoft SPTAG [14].We also compare Milvus with three industrial-strength commercialsystems (with latest version as of July 2020) anonymized as SystemA, B, and C for commercial reasons. Since Milvus is implementedon top of Faiss [3,35], we also present the performance comparisonby evaluating the algorithmic optimizations in Milvus ( Sec. 7.4).Evaluation metrics.We use the recall to evaluate the accuracyof the top-:results returned by a system where:is 50 by default.Speci￿cally, let(be the ground-truth top-:result set and(0be thetop-:results from a system, then the recall is de￿ned as|(\(0|/|(|.Besides that, we also measure the throughput of a system by issuing10,000 random queries to the datasets.7.2Comparing with Prior SystemsIn this experiment, we compare Milvus against prior systems interms of recall and throughput. We use the ￿rst 10 million vectors from each dataset (referred to as SIFT10M and Deep10M) becauseprior systems are slow in building indexes and executing querieson billion-scale datasets. Note that we also evaluate Milvus onthe full billion-scale vectors in Sec. 7.3 to demonstrate the systemscalability. Except for the three commercial systems (A, B, and C)that the minimum con￿guration requires multiple nodes, we run allother systems (including Milvus) in a single node. Speci￿cally, werun System A and C on two nodes (with 64GB memory per node);System B on four nodes (with 128GB memory per node).In this experiment, we use two indexes IVF_FLAT and HNSWwhenever possible since both are supported by most systems, al-though Milvus supports more indexes.Figure 8 shows the results on IVF indexes (i.e., quantization-based indexes). Overall, Milvus (even CPU version) signi￿cantlyoutperforms existing systems by up to two orders of magnitudewhile keeping the similar recall. In particular, Milvus is 6.4⇥⇠27.0⇥faster than Vearch; 153.7⇥faster than System B even if System Bruns on four nodes;114.7⇥⇠11.5⇥faster than System C evenif System C runs on two nodes; 1.3⇥⇠2.1⇥faster than SPTAG(tree-based index). But SPTAG cannot achieve very high recall (e.g.,0.99) as Milvus does and also SPTAG takes 14⇥more memory thanMilvus (17.88GB vs. 1.27GB).12The GPU version of Milvus is evenfaster since data can ￿t in the GPU memory in this setting. Weomit the results of System B on Deep10M since it only supportsthe Euclidean distance metric. We also omit the results of Vearchon GPU because there are multiple bugs in building indexes thattheir engineers were still ￿xing by the time of paper writing.13Wedefer the results of System A to Figure 9 since it only supports theHNSW index.The performance advantage of Milvus comes from a few factorsin addition to engineering optimizations. (1) Milvus introduces ￿ne-grained parallelism that supports both inter-query and intra-queryparallelism to best leverage multi-core CPUs. (2) Milvus developscache-aware and SIMD-aware optimizations to reduce CPU cachemisses and leverage wide SIMD instructions. (3) Milvus optimizesfor the hybrid execution between GPU and CPU.Figure 9 shows the results on the HNSW index of each system.Milvus outperforms existing systems by a large margin. Speci￿cally,it is 15.1⇥⇠60.4⇥faster than Vearch; 8.0⇥⇠17.1⇥faster than System A; 7.3⇥⇠73.9⇥faster than System C. We omit System Aon Deep10M because System A does not support inner product. Weomit also System C on Deep10M because the index building fails tocomplete after more than 100 hours.7.3ScalabilityIn this experiment, we evaluate the scalability of Milvus in terms ofdata size and the number of servers. We use the IVF_FLAT indexingon the SIFT1B dataset that includes 1 billion vectors.Figure 10a shows the results on a single node of ecs.re6.26xlarge(104 vCPUs and 1.5TB memory) that can ￿t the entire data in mem-ory. As the data increases, the throughput gracefully drops pro-portionally. Figure 10b shows the scalability of distributed Mil-vus. The data is sharded among the nodes where each node isof ecs.g6e.13xlarge (52 vCPUs and 192GB memory). As the num-ber of nodes increases, the throughput increases linearly. Notethat we observe that Milvus achieves higher throughput on theecs.g6e.13xlarge instance than the ecs.re6.26xlarge instance dueto the higher competition on the shared CPU caches and memorybandwidth among more cores.7.4Evaluation of OptimizationsFigure 11 shows the impact of cache-aware design on two CPUswith di￿erent L3 cache sizes: 12MB (Intel Core i7-8700 3.2GHz) and35.75MB (Xeon Platinum 8269 Cascade 2.5GHz).We set the querybatch size as 1000 and vary the data size (i.e., the number of vectors)from 1000 to 10 million.It shows that the cache-aware design canachieve a performance improvement up to 2.7⇥and 1.5⇥when thecache size is 12MB and 35.75MB, respectively.Figure 12 shows the impact of SIMD-aware optimizations fol-lowing the experimental setup in Figure 11. It compares the perfor-mance of AVX2 and AVX512 on the Xeon CPU. Figure 12 demon-strates that AVX512 is roughly 1.5⇥faster than AVX2.Figure 13 evaluates the e￿ciency of the hybrid algorithm SQ8H(Algorithm 1) in Milvus on SIFT1B where data cannot ￿t into GPU memory. We compare SQ8H with SQ8 on pure CPU and pure GPU.It shows that GPU SQ8 is slower than CPU SQ8 due to the datatransfer. As the query batch size increases, the performance gapbetween GPU and CPU becomes smaller since more computationsare pushed to the GPU. In all cases, SQ8H is faster than runningSQ8 on pure CPU and pure GPU. That is because SQ8H only storesthe centroids in GPU memory to execute the ￿rst step and allowsthe CPU to execute the second step so that there is no any datasegment transferred to GPU memory on the ￿y.7.5Evaluating Attribute FilteringWe de￿ne the query selectivity as the percentage of entities thatfails the attribute constraintCfollowing [65]. Thus, a higherselectivity means that a smaller number of entities can passC.Regarding the dataset, we extract the ￿rst 100 million vectors fromSIFT1B and augment each vector with an attribute of a randomvalue ranging from 0 to 10000. We follow [65] to generate twoscenarios of di￿erent:(50 and 500) and recall (0.95 and 0.85).Figure 14 shows the results with varying query selectivity. Forthe strategy A, its performance increases as the selectivity increasesbecause the number of examined vectors decreases. The strategyB is insensitive to the selectivity since the bottleneck is vectorsimilarity search. The strategy C is slower than the strategy B sinceit requires to check\times of the vectors where\is 1.1 in thisexperiment. The strategy D outperforms A, B, and C since it uses acost-based approach to choose the best between the three. Our newapproach, i.e., the strategy E, signi￿cantly outperforms the strategyD by up to 13.7⇥due to the partitioning.Figure 15 compares Milvus against System A, B, C, and Vearchin terms of attribute ￿ltering. It shows that Milvus outperformsthose systems by 48.5⇥to 41299.5⇥. Note that we omit the resultsof System B in Figure 15b because its parameters are ￿xed by thesystem that users are not allowed to change.7.6Evaluating Multi-vector Query ProcessingIn this experiment, we evaluate the algorithms for multi-vectorquery processing. Since SIFT1B and Deep1B only contain one vector per entity, we then use another dataset called Recipe1M [50,56] thatincludes more than one million cooking recipes and food images.Thus, each entity is described by two vectors: text vector (i.e., recipedescription) and image vector (i.e., food image). We randomly pickup 10000 queries from the dataset and set:as 50 in this experiment.We use the IVF_FLAT indexing in this experiment.Besides that, weuse weighted sum as the aggregation function.Figure 16a shows the results where the similarity metric is Eu-clidean distance. We compare the standard NRA algorithm of di￿er-ent:(50 and 2048) and our iterative merging (“IMG” for short) ofdi￿erent:0(4096, 8192, and 16384). It shows that the standard NRAapproach is either slow or produces low recall. In particular, theNRA-50 approach is fast but the recall is only 0.1. The NRA-2048increases the recall a bit (up to 0.5), but the performance is lowwhile our iterative merging algorithm (with:0being 4096) is 15⇥faster than NRA-2048 with a similar recall. That is because IMGdoes not need to invoke the vector query processing every timeand also it has lower maintenance cost of heaps.Figure 16b shows the results on the inner product metric. Wecompare the iterative merging (IMG-4096 and IMG-8192) with vec-tor fusion. It shows that vector fusion is 3.4⇥⇠5.8⇥faster since itonly needs to issue a single top-:vector similarity search.8RELATED WORKVector similarity search (a.k.a high-dimensional nearest neigh-bor search) is an extensively studied topic both for approximatesearch (e.g., [7,41]) and exact search (e.g., [38,42]). This workfocuses on approximate search in order to achieve high perfor-mance. Prior works on approximate search can be roughly classi-￿ed in four categories: LSH-based [23,23,24,32,40,44,45,48,73],tree-based [17,46,54,57], graph-based [20,43,49,61,72], andquantization-based [3,6,22,27,33,35]. However, those works are all about indexes while Milvus is a full-￿edged vector data manage-ment system including indexes, query engine, GPU engine, storageengine, and distributed system. Moreover, Milvus’s extensible in-dex framework can easily incorporate those indexes as well as anynew index if necessary. There are also open-source libraries forvector similarity search, e.g., Faiss [35] and SPTAG [14]. But theyare libraries not systems. We summarize the di￿erences in Table 1.Recent industrial-strength vector data management systems suchas Alibaba PASE [68] and Alibaba AnalyticDB-V [65] are not par-ticularly optimized for vectors. Their approach is to extend therelational database to support vectors. As a result, the performancesu￿ers severely as demonstrated in experiments. Specialized vectorsystems like Vearch [39] are not suitable for billion-scale data andVearch is signi￿cantly slower than Milvus.There are also GPU-based vector search engines, e.g., [35,72].Of which, [72] optimizes HNSW for GPU but it assumes data to besmall enough to ￿t into GPU memory. Faiss [35] also supports GPU,but it loads the whole data segments on demand if data cannot ￿tinto GPU memory, leading to low performance. Instead, Milvusdevelops a new hybrid index (SQ8H) that combines the best ofthe GPU and CPU without loading data on the ￿y for fast queryprocessing.This work is relevant to the trend of building specialized dataengines since one size does not ￿t all [60], e.g., specialized graphengine [18], IoT engine [21], time series database [55], and scienti￿cdatabase [59]. In this regard, Milvus is a specialized data engine formanaging vector data.9CONCLUSIONIn this work, we share our experience in building Milvus over thelast few years at Zilliz.Milvus has been adopted by hundreds ofcompanies and is currently an incubation-stage project at the LF AI& Data Foundation. Looking forward, we plan to leverage FPGA toaccelerate Milvus. We have implemented the IVF_PQ indexing onFPGA and the initial results are encouraging. Another interestingyet challenging direction is to architect Milvus as a cloud-nativedata management system and we are currently working on it.ACKNOWLEDGMENTSMilvus is a multi-year project that involves many engineers atZilliz. In particular, we thank Shiyu Chen, Qing Li, Yunmei Li,Chenglong Li, Zizhao Chen, Yan Wang, and Yunying Zhang fortheir contributions. We also thank Haimeng Cai and Chris Warnockfor proofreading the paper. Finally, we would like to thank WalidG. Aref and the anonymous reviewers for their valuable feedback.
