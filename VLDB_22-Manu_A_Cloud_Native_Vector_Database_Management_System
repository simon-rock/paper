Manu: A Cloud Native Vector Database Management System


PVLDB Artifact Availability:The source code, data, and/or other artifacts have been made available athttps://github.com/milvus-io/milvus/tree/2.0.1  INTRODUCTIONAccording to IDC, unstructured data, such as text, images, and video,took up about 80% of the 40,000 exabytes of new data generated in2020, their percentage keeps rising due to the increasing amountof human-generated rich media [47]. With the rise of learning-based embedding models, especially deep neural networks, usingembedding vectors to manage unstructured data has become com-monplace in many applications such as e-commerce, social media,and drug discovery [48,62,67]. A core feature of these applica-tions is that they encode the semantics of unstructured data into ahigh-dimensional vector space. Given the representation power ofembedding vectors, operations like recommendation, search, andanalysis can be implemented via similarity-based vector search. Tosupport these applications, many specialized vector databases arebuilt to manage vector data [10, 12, 17‚Äì19, 80].In 2019, we open sourced Milvus [80], our previous vector data-base, under the LF AI & Data Foundation. Since then, we collectedfeed-backs from more than 1200 industry users and found that someof the design principles adopted by Milvus are not suitable. Milvusfollowed the design principles of relational databases, which areoptimized for either transaction [51] or analytical [80] workloads,and focused on functionality supports (e.g., attribute filtering andmulti-vector search) and execution efficiency (e.g., SIMD and cacheoptimizations). However, vector database applications have differ-ent requirements in the following three aspects, which motivatesus to restructureManufrom scratch with focuses on a cloud-nativearchitecture.‚Ä¢Support for complex transactions is not necessary.Insteadof decomposing entity representations into different fields ortables, learning-based models encode complex and hybrid datasemantics into a single vector. As a result, multi-row or multi-table transactions are not necessary; row-level ACID is sufficientfor the majority of vector database applications.‚Ä¢A tunable performance-consistency trade-off is important.Different users have different consistency requirements; someusers prefer high throughput and eventual consistency, whileothers require some level of guaranteed consistency, i.e., newlyinserted data should be visible to queries either immediately orwithin a pre-configured time. Traditional relational databasesgenerally support either strong consistency or eventual consis-tency; there is little to no room for customization between these two extremes. As such,tunableconsistency is a crucial attributefor cloud-native vector databases.‚Ä¢High hardware cost calls for fine-grained elasticity.Somevector database operations (e.g., vector search and index build-ing) are computationally intensive, and hardware accelerators(e.g. GPUs or FPGAs) and/or a large working memory are re-quired for good performance. However, depending on applicationtypes, workload differs amongst database functionalities. Thus,resources can be wasted or improperly allocated if the vectordatabase does not have fine-grained elasticity. This necessitatesa careful decoupling of functional and hardware layers; system-level decoupling such as separation of read and write logic isinsufficient, elasticity and resource isolation should be managedat the functionalities level rather than the system level.In summary, modern vector databases should have tunable con-sistency, functionality-level decoupling, and per-component scal-ability. Following the design principles of traditional relationaldatabases makes achieving these design goals extremely difficult, ifnot impossible. A key opportunity for achieving these design goalslies in the potential for relaxing transaction complexity.Manufollows the ‚Äúlog as data‚Äù paradigm. Specifically,Manustruc-tures the entire system as a group of log publish/subscribe micro-services. The write-ahead log (WAL) and inter-component mes-sages are published as ‚Äúlogs", i.e., durable data streams that can besubscribed. Read-side components, such as search and analyticalengines, are all built as log subscribers. This architecture providesa simple yet effective way to decouple system functionalities; itenables the decoupling of read from write, stateless from stateful,and storage from computing. Each log entry is assigned a globalunique timestamp, and special log entries called time-tick (simi-lar to watermarks in Apache Flink [25]) are periodically insertedinto each log channel signaling the progress of event-time for logsubscribers. The timestamp and time-tick form the basis of thetunable consistency mechanism and multi-version consistency con-trol (MVCC). To control the consistency level, a user can specifya tolerable time lag between a query‚Äôs timestamp and the latesttime-tick consumed by a subscriber.Additionally, we extensively optimizeManufor performanceand usability.Manusupports various indexes for vector search, in-cluding vector quantization [21,33,36,82], inverted index [23], andproximity graphs [32]. In particular, we tailor the implementationsto better utilize the parallelization capabilities of modern CPUsand GPUs along with the improved read/write speeds of SSDs overHDDs.Manualso integrates refactored functionalities from Mil-vus [80], such as attribute filtering and multi-vector search. More-over, build a visualization tool that allows users to track the perfor-mance ofManuin real time and include an auto-configuration toolthat recommends indexing algorithm parameters using machinelearning.To summarize, this paper makes the following contributions:‚Ä¢We summarize lessons learned from communicating with over1200 industry users over three years. We shed light on typicalapplication requirements of vector databases and show how theydiffer from those of traditional relational databases. We thenoutline the key design goals that vector databases should meet. ‚Ä¢We introduceManu‚Äôs key architectural designs as a cloud nativevector database, building around the core design philosophy ofrelaxing transaction complexity in exchange for tunable consis-tency and fine-grained elasticity.‚Ä¢We  present  important  usability  and  performance-related  en-hancements, e.g., high-level API, a GUI tool, automatic parameterconfiguration, and SSD support.The rest of the paper is organized as follows. Section 2 pro-vides background on the requirements and design goals of vectordatabases. Section 3 dives deep intoManu‚Äôs design. Section 4 high-lights the key features for usability and performance. Section 5discusses representative use cases forManu. Section 6 review re-lated works. Section 7 concludes the paper and outlines futurework.2  BACKGROUND AND MOTIVATIONConsider video recommendation as a typical use case of vectordatabases. The goal is to help users discover new videos based ontheir personal preferences and previous browsing history. Usingmachine learning models (especially deep neural networks), fea-tures of users and videos, such as search history, watch history,age, gender, video language, and tags are converted to embeddingvectors. These models are carefully designed and trained to encodethe similarity between user and video vectors into a common vec-tor space. Recommendation is conducted by retrieving candidatevideos from the collection of video vectors via similarity scoreswith respect to the specified user vector. The system also needsto handle updates to vectors when new videos are updated, somevideos are deleted and the embedding model is changed.Video recommendation and other applications of vector databasescan involve hundreds of billions of vectors with daily growth athundred-million scale, and serve million-scale queries per second(QPS). Existing DBMSs (e.g., relational databases [9,11], NoSQL [75,85], NewSQL [39,73]) were not built to manage vector data on thatscale. Moreover, the underlying data management requirements oftheir applications differ greatly from vector database applications.First, when compared with relational databases, both the archi-tecture and theory of vector databases are far from mature. A keyreason for this is that AI- and data-driven applications are stillin a state of constant evolution, thereby necessitating continuedarchitectural and functionality changes to vector databases as well.Second, complex transactions are unnecessary for vector databases.In the above example, the recommendation system encodes all se-mantic features of users and videos into standalone vectors asopposed to multi-row or multi-column entity fields in a relationaldatabase. As a result, row-level ACID is sufficient; multi-table oper-ations (such as joins) are inessential.Third, vector database applications need a flexible performance-consistency trade-off. While some applications adopt a strong oreventual consistency model, there are others that fall between thetwo extremes. Users may wish to relax consistency constraints inexchange for better system throughput. In the video recommen-dation example, observing a newly uploaded video after severalseconds is acceptable but keeping users waiting for recommenda-tion harms user experience. Thus, the application can configure the allowed maximal delay for the video updates in order to improvesystem throughput.Fourth, vector databases have more stringent and diversifiedhardware requirements compared with traditional databases. Thisis attributed to three reasons. First, vector database operationsare computation-intensive, and thus hardware accelerators suchas GPUs are critical for computing functionalities such as searchand indexing. Second, accesses to vector data (e.g., search or up-date) generally have poor locality, thereby requiring large RAMfor good performance. Third, different applications vary signifi-cantly in their resource demands for the system functionalities.Core functionalities of a vector database include data insertion,indexing, filtering, and vector search. Applications such as videorecommendation require online insertion and high concurrencyvector search. In contrast, for interactive use cases such as drug dis-covery, offline data ingestion and indexing are generally acceptable.Although interactive applications usually require lower throughputthan recommendation systems, they have high demands for real-time filtering, similarity-based vector search, and hybrid queries.The high hardware costs as well as diverse workload features callfor fine-grained elasticity.The key design goals ofManuare summarized below; thesedesign goals not only fully encompass the above characteristics butalso share some common goals with generic cloud-based databases.‚Ä¢Long-term evolvability:Overall system complexity must becontrolled for the continuous evolution ofManu‚Äôs functionalities.Without the need to support complex transactions, there lies anopportunity to model all the event sequences (such as WAL andinter-component messages) as message queues to cleanly decou-ple the entire system. In this way, individual components canevolve, be added, or be replaced easily with minimal interferenceto other components. This design echos large-scale data analyticplatforms, which often rely on data streaming systems such asKafka to connect system components.‚Ä¢Tunable consistency:To enable flexible consistency-performancetrade-off,Manushould introducedelta consistencythat falls be-tween strong consistency and eventual consistency, where a readoperation returns the last value that was produced at most deltatime units preceding itself. It‚Äôs worth noting that strong consis-tency and eventual consistency can be realized as special casesof this model, with delta being zero and infinity, respectively.‚Ä¢Good elasticity:Workload fluctuations can cause different loadson individual system components. In order to dynamically al-locate compute resources to high-load tasks, components mustbe carefully decoupled, taking both functionality and hardwaredependencies into consideration. System elasticity and resourceisolation should be managed at the component-level rather thanat the system-level (e.g. decoupling indexing from querying ver-sus decoupling read from write).‚Ä¢High availability:Availability is a must-have for modern cloud-based applications;Manumust isolate system failures at thecomponent level and make failure recovery transparent.‚Ä¢High performance:Query processing performance is key tovector databases. For good performance, implementations to beextensively optimized for hardware. Moreover, the framework should be carefully designed so as to minimize system overheadsfor query serving.‚Ä¢Strong adaptability:Our customers use vector databases in avariety of environments, ranging from prototyping on laptops tolarge-scale deployments on the cloud. A vector database shouldprovide consistent user experience and reduce code/data migra-tion overhead across environments.3  THE MANU SYSTEMIn this section, we begin by first introducing the basic concepts ofManu. Next, we present the system designs, including the overallsystem architecture, the log backbone, and howManuconductsvector searches and builds vector search indexes.3.1  Schema, Collection, Shard, and SegmentSchema:The basic data types ofManuare vector, string, boolean,integer, and floating point. A schema example is given in Figure 1.Suppose each entity consists of five fields and corresponds to aproduct on an e-commerce platform. ThePrimary keyis the IDof the entity. It can either be an integer or a string. If users donot specify this field, the system will automatically add an integerprimary key for each entity. TheFeature vectoris the embedding ofthe product. TheLabelis the category of the product, such as food,book, and cloth. TheNumerical attributeis a float or an integerassociated with the product, such as price, weight, or productiondate.Manusupports multiple labels and numerical attributes ineach entity. Note that these fields are used for filtering, rather thanjoin or aggregation. TheLogical sequence number(LSN) is a systemfield hidden from users.Collection:A Collection is a set of entities similar to the concept oftables in relational databases. For example, a collection can containall the products of an e-commerce platform. The key difference isthat collections have no relations with each other; thus, relationalalgebra, such as join operations, are not supported.Shard:The Shard correspondence to insertion/deletion channel.Entities are hashed into multiple shards based on their primary keysduring insertion/deletion.Manu‚Äôs data placement unit is segmentrather than shard.1Segment:Entities from each shard are organized into segments. Asegment can be in either agrowingorsealedstate. Sealed segmentsare read-only while growing segments can accept new entities. Agrowing segment will switch to sealed state when it reaches a prede-fined size (set to 512MB by default) or if a period of time has passedwithout an insertion (e.g., 10 seconds). As some segments may be small (e.g., when insertion has a low arrival rate),Manumergessmall segments into larger ones for search efficiency.3.2  System ArchitectureManuadopts a service-oriented design [64] to achieve fine-graineddecoupling among the system components. As shown in Figure 2,from top to bottom,Manuhas four layers, i.e.,access layer,coordi-nator layer,worker layer, andstorage layer.Access layerconsists of stateless proxies that serve as the userendpoints. They work in parallel to receive requests from clients,distribute the requests to the corresponding processing components,and aggregate partial search results before returning to clients. Fur-thermore, the proxies cache a copy of the metadata for verifying thelegitimacy of search requests (e.g., whether the collection to searchexists). Search request verification is lightweight and moving it tothe proxies has two key benefits. First, requests that fail verificationare rejected early, thus lowering the load on other systems compo-nents. Second, it reduces the number of routing hops for requests,thus shortening request processing latency.Coordinator layermanages system status, maintains metadata ofthe collections, and coordinates the system components for process-ing tasks. There are four coordinators, each responsible for differenttasks.Root coordinatorhandles data definition requests, such ascreating/deleting collections, and maintains meta-information ofthe collections.Data coordinatorrecords detailed information aboutthe collections (e.g., the routes of the segments on storage), andcoordinates the data nodes to transform data update requests intobinlogs [4].Query coordinatormanages the status of the querynodes, and adjusts the assignment of segments (along with relatedindexes) to query nodes for load balancing.Index coordinatormain-tains meta-information of the indexes (e.g., index types and storageroutes), and coordinates index nodes in index building tasks. Acoordinator can have multiple instances (e.g., one main and twobackups) for reliability. As vector databases usually do not havethe cross table operations that relational databases have, differentcollections can be served by separate coordinator instances forthroughput.Worker layerconducts the actual computation tasks. The workernodes are stateless‚Äîthey fetch read-only copies of data to conducttasks and do not need to coordinate with each other. This ensures that computation intensive (thus expensive) worker nodes can beeasily scaled on demand. We use different worker nodes for dif-ferent tasks, i.e.,query nodesfor query processing,index nodesforindex building, anddata nodesfor log archiving. Due to the fact thatthe workloads for different computation tasks vary significantlyover time and across applications, each worker type can scale inde-pendently. This design also achieves resource isolation as differentcomputation tasks have different QoS requirements.Storage layerpersists system status, metadata, the collections,and associated indexes.Manuuses etcd [7] (a key-value store)to host system status and metadata for the coordinators as etcdprovides high availability with its leader election mechanism forfailure recovery. When metadata is updated, the updated data is firstwritten to etcd, and then synchronized to coordinators. Since thevolume of other data (e.g., binlog, data, index) is large,ManuusesAWS S3 [13] (an object storage) for persistence due to its highavailability and low cost. The API of many other object storagesystems is compatible with AWS S3. This allowsManuto easilyswap storage engines, if necessary. At present, storage enginesincluding AWS S3, MinIO [8], and Linux file system are supported.Note that the high latency that comes with object storage is not aperformance bottleneck as the worker nodes conduct computationtasks on in-memory, read-only copies of data.3.3  The Log BackboneThe log system is the backbone ofManu, which connects the de-coupled system components. As shown in Figure 3,Manuexposesthe write-ahead log (WAL) and binlog as backbone services. TheWAL is the incremental part of system log while the binlog is thebase part; they complement each other in delay, capacity and cost.Loggers are entry points for publishing data onto the WAL. Datanodes subscribe to the WAL and convert the row-based WALs intocolumn-based binlogs. All read-only components such as indexnodes and query nodes are independent subscribers to the log ser-vice to keep themselves up-to-date. This architecture completelydecouples the write and read components, thus allowing the compo-nents (e.g., WAL, binlog, data nodes, index nodes and query nodes)to scale independently.Manurecords all the requests that change system state to thelog, including data definition requests (e.g., create/delete collection),data manipulation requests (e.g., insert/delete a vector), and sys-tem coordination messages (e.g., load/dump a collection to/frommemory). Note that vector search requests are not written to thelog as they are read-only operations and do not change system state. We use logical logs instead of physical logs, as logical logsfocus on event recording, rather than describing the modificationsto physical data pages. This allows the subscribers to consume thelog data in different ways depending on their functions.Figure 4 illustrates the detailed architecture of the log system.For the sake of clarity, we only illustrate the parts related to insertrequests. The loggers are organized in a hash ring, and each loggerhandles one or more logical buckets in the hash ring based onconsistent hashing. Each shard corresponds to a logical bucket inthe hash ring and a WAL channel. Each entity in insert requestsis hashed to a shard (and thus channel) based on their ID. Whena logger receives a request, it will first verify the legibility of therequest, assign an LSN for the logged entity by consulting thecentral time service oracle (TSO), determine the segment the entityshould go to, and write the entity to WAL. The logger also writesthe mapping of the new entity ID to segment ID into a local LSMtree and periodically flushes the incremental part of the LSM treeto object storage, which keeps the entity to segment mapping usingthe SSTable format of RocksDB. Each logger caches the segmentmapping (e.g., for checking if the entity to delete exists) for theshards it manages by consulting the SSTable in object storage.The WAL is row-based and read in a streaming manner for lowdelay and fine-grained log pub/sub. It is implemented via a cloud-based message queue such as Kafka or Pulsar. We use multiplelogical channels for the WAL in order to prevent different types ofrequests from interfering with each other, thus achieving a highthroughput. Data definition requests and system coordination mes-sages use their own channels while data manipulation requestshashed across multiple channels to increase throughput.Data nodes subscribe to the WAL and convert the row-basedWALs into column-based binlogs. Specifically, values from the samefield (e.g., attribute and vector) from the WALs are stored togetherin a column format in binlog files. The column-based nature ofbinlog makes it suitable for reading per field values in batches, thusincreasing storage and IO efficiency. An example of this efficiencycomes with the index nodes. Index nodes only read the requiredfields (e.g., attribute or vector) from the binlog for index buildingand thus are free from the read amplifications.System coordination:Inter-component messages are also passedvia log, e.g., data nodes announce when segments are written tostorage and index nodes announce when indexes have been built.This is because the log system provides a simple and reliable mecha-nism for broadcasting system events. Moreover, the time semanticsof the log system provide a deterministic order for coordinationmessages. For example, when a collection should be released frommemory, the query coordinator publishes the request to log, anddoes not need to confirm whether the query nodes receive the mes-sage or handle query node failure. The query nodes independentlysubscribe to the log and asynchronously release segments of thecollection.
3.4  Tunable ConsistencyWe adopt a delta consistency model to enable flexible performance-consistency trade-offs, which guarantees a bounded staleness ofdata seen by search queries. Specifically, the data seen by a querycan be stale for up to delta time units, with respect to time of the last data update, where delta is an user-specified ‚Äústaleness tolerance‚Äùgiven in virtual time.In practice, users prefer to define temporal tolerance as physicaltime, e.g., 10 seconds.Manuachieves this by making the LSN as-signed to each request extremely close to physical time.Manuusesa hybrid logical clock in the TSO to generate timestamps. Eachtimestamp has two components: a physical component that tracksphysical time, and a logical component that tracks event order. Thelogical component is needed since multiple events may happen atthe same physical time unit. Since a timestamp is used as a request‚ÄôsLSN, the value of the physical component indicates the physicaltime when the request was received byManu.For a log subscriber, e.g., a query node, to run the delta consis-tency model, it needs to know three things: (1) the user-specifiedstaleness toleranceùúè, (2) the time of the last data update, and (3) theissue time of the search request. In order to let each log subscriberknow (2), we introduce a time-tick mechanism. Special control mes-sages called time-ticks (similar to watermarks in Apache Flink [25])are periodically inserted into each log channel (for example, WALchannel) signaling the progress of data synchronization. Denotethe latest time-tick a subscriber consumed asùêøùë†and the issue timeof a query asùêøùëü, ifùêøùëü‚àíùêøùë†<ùúèis not satisfied, the query node willwait for the next time-tick before executing the query.Note that strong consistency and eventual consistency are twospecial cases of delta consistency, where delta equals to 0 and infin-ity, respectively. To the best of our knowledge, our work is the firstto support delta consistency in a vector database.

3.5  Index BuildingSearching similar vectors in large collections by brute-force, i.e.,scanning the whole dataset, usually yields unacceptably long de-lays. Numerous indexes have been proposed to accelerate vectorsearch andManuautomatically builds user specified indexes. Ta-ble 1 summarizes the indexes currently supported byManu, andwe are continuously adding new indexes following the latest in-dexing algorithms. These indexes differ in their properties and usecases. Vector quantization (VQ) [33,44] methods compress vectors3552
to reduce memory footprint and the costs for vector distance/simi-larity computation. For example, scalar quantization (SQ) [90] mapseach dimension of vector (data types typically are int32 and float)to a single byte. Inverted indexes [68] group vectors into clusters,and only scan the most promising clusters for a query. Proximitygraphs [32,41,60] connect similar vectors to form a graph, andachieve high accuracy and low latency at the cost of high mem-ory consumption [53]. Besides vector indexes,Manualso supportsindexes on the attribute field of the entities to accelerate attribute-based filtering.There are two index building scenarios inManu, i.e.,batch in-dexingandstream indexing. Batch indexing occurs when the userbuilds an index for an entire collection (e.g., when all vectors areupdated with a new embedding model). In this case, the index co-ordinator obtains the paths of all segments in the collection fromthe data coordinator, and instructs index nodes to build indexesfor each segment. Stream indexing happens when users contin-uously insert new entities, and indexes are built asynchronouslyon-the-fly without stopping search services. Specifically, after a seg-ment accumulates a sufficient number of vectors, its resident datanode seals the segment and writes it to object storage as a binlog.The data coordinator then notifies the index coordinator, whichinstructs a index node to build index for the segment. The indexnode loads only the required column (e.g., vector or attribute) ofthe segment from object storage for indexing building to avoid readamplification. For entity deletions,Manuuses a bitmap to recordthe deleted vectors and rebuilds the index for a segment when asufficient number of its entities have been deleted. In both batchand stream indexing scenarios, after the required index is built for asegment, the index node persists it in the object storage and sendsthe path to the index coordinator, which notifies the query coordi-nator so that query nodes can load the index for processing queries.The index coordinator also monitors the status of the index nodesand shuts down idle index nodes to save costs. As vector indexesgenerally have sub-linear search complexity w.r.t. the number ofvectors, searching a large segment is cheaper than several smallsegments,Manubuilds joint indexes on multiple segments whenappropriate.

3.6  Vector SearchManusupportsclassical vector search,attribute filtering, andmulti-vector search. For classical vector search, the distance/similarityfunction can be Euclidean distance, inner product or angular dis-tance. Attribute filtering is useful when searching vectors similarto the query subject to some attribute constraints. For example, ane-commerce platform may want to find products that interest thecustomer and cost less than 100$.Manusupports three strategiesfor attribute filtering and uses a cost-based model to choose themost suitable strategy for each segment. Multi-vector search isrequired when an entity is encoded by multiple vectors, for exam-ple, a product can be described by both embeddings of its imageand embeddings of its text description. In this case, the similarityfunction between entities is defined as a composition of similarityfunctions on the constituting vectors.Manusupports two strategiesfor multi-vector search and chooses the one to use according to the entity similarity function. For more details about howManuhan-dles attribute filtering and multi-vector search, interested readerscan refer to Milvus [80].For vector search,Manupartitions a collection into segmentsand distributes the segments among query nodes for parallel exe-cution.2The proxies cache a copy of the distribution of segmentson query nodes by inquiring the query coordinator, and dispatchsearch requests to query nodes that hold segments of the searchedcollection. The query nodes perform vector searches on their localsegments without coordination using atwo-phase reduceprocedure.For a top-ùëòvector search request, the query nodes search their localsegments to obtain the segment-wise top-ùëòresults. These resultsare merged by each query node to form the node-wise top-ùëòresults.Then, the node-wise top-ùëòresults are aggregated by the proxy forthe global top-ùëòresults and returned to the application. To handlethe deletion of vectors, the query nodes use a bitmap to recordthe deleted vectors in each segment and filter the deleted vectorsfrom the segment-wise search results. Users can configureManutobatch search requests to improve efficiency. In this case, the proxiesorganize cache search requests if results of the previous batcheshave not been returned yet. In the cache, requests of the same type(i.e., target the same collection and use the same similarity function)are organized into the one batch and handled byManutogether.Manualso allows maintaining multiple hot replicas of a collectionto serve queries for availability and throughput.Query nodes obtain data from three sources, i.e., the WAL, theindex files, and the binlog. For data in the growing segments, querynodes subscribe to the WAL and conduct searches using bruteforce scan so that updates can be searched within a short delay. Adilemma for segment size is that larger size yields better searchefficiency once the index is built but brute force scan on growingsegment is also more costly. To tackle this problem, we divide eachsegment intoslices(each containing 10,000 vectors by default). Newdata are inserted into the slices sequentially, and after a slice isfull, a light-weight temporary index (e.g., IVF-FLAT) is built for it.Empirically, we observed that the temporary index brings up to 10Xspeedup for searching growing segments. When a segment changesfrom growing state to sealed state, its index will be built by an indexnode and then stored in object storage. After that, query nodes arenotified to load the index and replace the temporary index.Query nodes access the binlog for data when the distributionof segments among the query nodes changes, which may happenduring scaling, load-balancing, query node failure and recovery.Specifically, the query coordinator manages the segment distribu-tion and monitors the query nodes for liveness and workload tocoordinate failure recovery and scaling. On failure recovery, thesegments and their corresponding indexes (if they exist) handledby failed query nodes are loaded to the healthy ones.3In the caseof scaling down, a query node can be removed once other querynodes load the indexes for the segments it handles from the objectstorage. When scaling up, the query coordinator assigns some of the segments to the newly added nodes. A new query node can joinafter it loads the assigned segments, and existing query nodes canrelease the segments no longer handled by them. The query coordi-nator also balances the workloads (and memory consumption) ofthe query nodes by migrating segments. Note thatManudoes notensure that segment redistribution is atomic, and a segment can re-side on more than one query node. This does not affect correctnessas the proxies remove duplicate result vectors for a query.

4  FEATURE HIGHLIGHTSIn this part, we introduce several key features ofManufor usabilityand performance.
4.1  Cloud Native and AdaptiveThe primary design goal ofManuis to be a cloud native vectordatabase such that it fits well into cloud-based data pipelines. Tothis end,Manudecouples system functionalities into storage, coor-dinators, and workers in the overall design. For storage,Manuusesa transaction KV for metadata, message queues for logs, and an ob-ject KV for data, which are all general storage services provided bymajor cloud vendors and thus enables easy deployment. For coordi-nators that manage system functionalities,Manuuses the standardone main plus two hot backups configuration for high availability.For workers,Manudecouples vector search, log archiving and in-dex building tasks for component-wise scaling, a model suitable forcloud-based on-demand resource provisioning. The log backboneallows the system components to interact by writing/reading logsin their own ways. This enables the system components to evolveindependently and makes it easy to add new components. The logbackbone also provides consistent time semantics in the system,which are crucial for deterministic execution and failure recovery.Our customers use vector databases in the entire life-cycle oftheir applications. For example, an application usually starts withdata scientists conducting proof of concept (PoC) on their personalcomputers. Then, it is migrated to dedicated clusters for testingand finally deployed on the cloud. Thus, to reduce migration costs,our customers expect vector databases to adapt to different deploy-ment scenarios while providing a consistent set of APIs. To thisend,Manudefines unified interface for the system componentsbut provides different invocation methods and implementations fordifferent platforms. For example, on cloud, local cluster and per-sonal computer,Manuuses cloud service APIs, remote procedurecall (RPC) and direct function calls to invoke system functionali-ties, respectively. The object KV can be the local file system (e.g.,MinIO [8]) on personal computers, and S3 on AWS. Thus,Manu applications can migrate with little or no change across differentdeployment scenarios.

4.2  Good UsabilityData pipelines interact withManuin simple ways: vector collec-tions, updates for vector data and search requests are fed toManu,andManureturns the identifiers of the search results for eachsearch request, which can be used to retrieve objects (e.g.. images,advertisements, movies) in other systems. Because different usersadopt different programming languages and development envi-ronments,Manuprovides APIs in popular languages includingPython, Java, Go, C++, along with RESTful APIs. As an example, weshow key commands of the Python-based PyManuAPI in Table 2,which uses the object-relational mapping (ORM) model and mostcommands are related to thecollectionclass. As shown in Table 2,PyManuallows users to manage collections and indexes, updatecollections, and conduct vector searches. Thesearchcommand isused for similarity-based vector search while thequerycommandis mainly used for attribute filtering. We show an example of con-ducting top-ùëòvector search by specifying the parameters inparamsin as follows.query_param = {"vec": [[0.6, 0.3, ..., 0.8]],"field": "vector","param": {"metric_type": "Euclidean"},"limit": 2,"expr": "product_count > 0",}res = collection.search(**query_param)In the above example, the search request provides a high dimen-sional vector[0.6,0.3, ...,0.8]as query and searches thefeaturevectorfield of the collection. The similarity function is Euclideandistance and the targets are the top-2 similar vectors in the collec-tion (i.e., withlimit=2).For easy system management,Manuprovides a GUI tool calledAttu, for which a screen shot is shown in Figure 5. In thesystem view,users can observe overall system status including queries processedper second (QPS), average query latency, and memory consumptionon the top of screen. By clicking a specific service (e.g., data service),users can view detailed information of the worker nodes for theservice on the side. We also allow users to add and drop workernodes with mouse clicks. In thecollection view, users can check thecollections in the system, load/dump collections to/from memory,delete/import collections, check the index built for the collections,and build new indexes. In thevector search view, users can checkthe search traffic and performance on each collection, configure the index and search parameters to use for each collection. The vectorsearch view also allows to issue queries for functionality test.For vector search, using different parameters for the indexes(e.g., neighbor sizeùëÄand queue sizeùêøfor HNSW [60]) yields dif-ferent trade-offs among cost, accuracy, and performance. However,even experts find it difficult to set proper index parameters as theparameters are interdependent and their influences vary acrosscollections.Manuadopts a Bayesian Optimization with Hyperband(BOHB) [31] method to automatically explore good index param-eter configurations. Users provide a utility function to score theconfigurations (e.g., according to search recall, query throughput)and set a budget to limit the costs of parameter search. BOHB startswith a group of initial configurations and evaluates their utilities.Then, Bayesian Optimization is used to generate new candidateconfigurations according to historical trials and Hyperband is usedto allocate budgets to different areas in the configuration space.The idea is to prioritize the exploration of areas close to high util-ity configurations to find even better configurations.Manualsosupports sampling a subset of the collection for the trails to reducesearch costs. We are still improving the automatic parameter searchmodule and plan to extend it to searching system configurations(e.g., the number and type of query nodes).

4.3  Time TravelUsers often need to rollback the database to fix corrupted data orcode bugs.Manuallows users to specify a target physical timeùëáfor database restore, and jointly uses checkpoint and log replay forrollback. We mark each segment with its progressùêøand periodi-cally checkpoints the segment map for a collection, which containsinformation (such a route, rather than data) of all its segments. Torestore the database at timeùëá, we read the closest checkpoint beforeùëá, load all segments in the segment map and replay the WAL log foreach segment from its local progressùêø. This design reduces storageconsumption as we do not write entire collection for each check-point. Instead, segments that have no changes are shared amongcheckpoints. The replay overhead is also reduced as each segmenthas its own progress. Users can also specify a expiration period todelete outdated log and segments to reduce storage consumption.

4.4  Hardware OptimizationsManucomes with extensively optimized implementations for CPU,GPU and SSD for efficiency. For more details about our CPU andGPU optimizations, interested readers can refer to Milvus [80]. SSD is 100x cheaper than dram and offers 10x larger bandwidththan HDD. thus,Manusupports using SSD to store large vectorcollections on cheap query nodes with limited dram capacity. thechallenge is that SSD bandwidth is still much smaller than dram,which may lead to low query processing throughput and thus ne-cessitates careful designs for storage layout and index structure. asSSD reads are conducted with 4kb blocks (i.e., reading less than4kb has the same cost as reading 4kb),Manuorganizes the vectorsinto buckets whose sizes are close to but smaller than 4kb.4this isachieved by conducting hierarchical k-means for the vectors andcontrolling the sizes of the clusters. each bucket is stored on 4kbaligned blocks on SSD for efficient read and represented by its k-means center in dram. these centers are organized using existingvector search indexes (e.g., ivf-flat, hnsw).vector search with SSD is conducted in two stages. first, wesearch the cluster centers in dram for the ones that are most similarto the query. then, the corresponding buckets are loaded fromSSD for scan. to reduce the amount of data fetched from SSD, wecompress the vectors using scalar quantization, which has negligibleinfluence on the quality of search results according to our trials.another problem is that k-means can put vectors similar to a queryinto several buckets but the centers of some buckets may not besimilar to the query, which leads to a low recall. to tackle thisproblem,Manuuses a strategy similar to multiple hash tables inlocality sensitive hashing [40]. hierarchical k-means is conductedby multiple times, each time assigning a vector to a bucket. thismeans that a vector is replicated multiple times in SSD and we indexall cluster centers for bucket search in dram.Manu‚Äôs SSD solutionwins track 2 (search with SSD) of the billion-scale approximatenearest neighbor search challenge at neurips‚Äô2021 [3]. tests resultsshow thatManu‚Äôs solution improves the recall of the competitionbaseline by up to 60% at the same query processing throughput.5we notice that another work adopts similar designs for SSD-basedvector search [26].

5  USE CASES AND EVALUATIONBefore introducing the use cases ofManu, we first compareManuwith Milvus, our previous vector database. Milvus adopts an even-tual consistency model and thus does not support the tunable con-sistency ofManu. To show the advantages brought byManu‚Äôs fine-grained functionality decomposition, we create a mixed work-load. Specifically, we start with an empty collection, insert vectorsat a fixed rate (e.g., 2k vectors per second), and measure the latencyfor search requests over time. BothManuand Milvus use 6 nodesand are properly configured for good performance. The results inFigure 6 show that the search latency of Milvus is significantlylonger thanManu, especially when insertion rates are high (e.g.,at 3k and 4k). Milvus has multiple read nodes, but only one writenode, to ensure eventual consistency. The write node responsiblefor data insertion and index construction, and thus write tasks andindex building tasks contend for resource. As a result, the indexbuilding latency is long and brute force search is used for a largeamount of data. In contrast, with dedicated index nodes,Manufinishes index building quickly and thus search latency remainslow over the entire period.

5.1  Overview of Use CasesWe classify our customers into 5 application domains in Figure 7and briefly elaborate them as follows.Recommendation:Platforms for e-commerce [78], music [76],news [54], video [27], and social network [43] record user-contentinteractions, and use the data to map users and contents to em-bedding vectors with techniques such as ALS [74] and deep learn-ing [49]. Finding contents of interest for a user is conducted bysearching content vectors having large similarity scores (typicallyinner product) with user vector.Multimedia:Multimedia contents (e.g., images, video and audio)are becoming increasingly popular, and searches for multimediacontents from large corpus are common online. The general prac-tice is to embed both user query and corpus contents into vectorsusing tools such as CNN [48] and RNN [86]. Searching multimediacontents is conducted by finding vectors similar to the user query.Language:Automatic questing answering and machine-based dia-logue attract much attention recently with products such as Siri [14]and Xiaoice [20], and searches for text contents is a general need.With models such as Word2Vec [62] and BERT [30], language se-quences are embedded into vectors such that retrieving language contents boils down to finding content vectors that are similar touser query.Security:Blocking spams and scanning viruses are important forsecurity. The common practice is to map spams and viruses intovectors using hashing [58] or tailored algorithms [38]. After that,suspicious spams and viruses can be checked by finding the mostsimilar candidates in the corpus for further check.Medicine:Many medical applications search for certain chemicalstructures and gene sequences for drug discovery or health riskidentification. With tools such as GNN [67] and LSTM [83], chemi-cal structures and gene sequences can be embedded into vectorsand their search tasks are cast into vector search.Full-fledged vector databases are necessary for the forgoing do-mains as they require much more complex functionality supportin addition to vector search. Specifically, as the vector datasetsare large and applications have high requirements for throughput,they need distributed computing with multiple nodes for scalability.The vectors are also continuously updated when new user/contentcomes, user behavior changes or the embedding model is updated.Since most of these applications serve end users, they require highavailability and durability. Some of our customers have deployedManuin their production environment, and they foundManusatis-factory in terms of usability, performance, elasticity, and adaptabil-ity. In what follows, we simulate some typical application scenariosof our customers to demonstrate the advantages ofManu.

5.2  Example Use CasesDue to business security, the names of the customers are anony-mous. For the experiments, we use two datasets widely used forvector search research, i.e., SIFT [5](with 128-dim vectors) andDEEP [2] (with 96-dim vectors), and extract sub-datasets with the re-quired sizes. By default, we use two query nodes, one data node andone index node forManu. Each worker node is an EC2 m5.4xlargeinstance running on Amazon Linux AMI version 5.4.129. For in-dex, we experiment with IVF-FLAT [45] and HNSW [60], whichare widely used in practice. When comparingManuwith othersystems, we always ensure that the systems use the same resourceand are properly configured. Due to time and expense limits, weare only able to compare with some vector databases in a subsetof the experiments. We search the top-50 most similar vectors foreach query, and ensure that average search recall is above 0.8 ifrecall is not reported explicitly.

E-commerce recommendation.Company A is a leading onlineshopping platform in China that mainly sells clothing and make-ups. They useManufor recommendation, and products are recom-mended to a user according to their similarity scores with the userembedding vector. They have three main requirements for vectordatabase: (1)high throughputas they need to handle the requestsof many concurrent costumers; (2)high qualitysearch results forgood recommendation effect; (3)good elasticityfor low costs astheir search requests have large fluctuations over time (peaks inevening but very low in midnight, very high at promotion events).In Figure 8, we compare the recall-throughput performance ofManuwith four popular open-source vector search systems (repre-sent as System A, System B, System C and System D), when usinga single node. We use Euclidean distance for SIFT and inner prod-uct for DEEP to test different similarity functions. As System Aonly supports its own graph-based index, System B and System Donly supports the HNSW index [60], we have only a single curvefor them in each plot. The results show thatManuconsistentlyoutperforms the baselines across different datasets and similarityfunctions. System C and System D achieve significantly lower queryprocessing throughput than Manu at the same recall. This is becausethat System C‚Äôs complex aggregation procedure for search resultsintroduces high overhead and System D is a disk-based solution.The performances of System A and System B are much better thanSystem C and System D but still inferior compared with Manu. Weconjecture this is because Manu has better implementations withoptimizations for CPU cache and SIMD.To test the elasticity ofManu, we use the search traffic of ane-commerce platform over one day period [16], which is plotted asthe purple curve in Figure 9. The results show that search workloadfluctuates violently over time, and the peak is much higher thanthe valley. We use SIFT100M as the dataset and Euclidean distanceas the similarity function.Manuis configured to reduce querynodes by 0.5x when search latency is shorter than 100ms and addquery nodes to 2x when search latency is over 150ms. The colors inFigure 9 indicate the number of query nodes used byManu, whichshows thatManuhas good elasticity to adapt to query workload.The black line reports the search latency and shows thatManucankeep search latency within the target range via scaling.Video deduplication.Company B is a video sharing website inEurope, on which users can upload videos and share with others. They find that there are many duplicate videos that result in highmanagement costs and thus conduct deduplication before archivingthe videos. They model a video as a set of its critical frames andencode each frame into a vector. They use vector search to findvideos in the corpus that are most similar to a new video andconduct further checking on the shortlisted videos to determine ifthe new video is a duplicate. They also use vector search to findvideos similar to those viewed by users for recommendation. Theyrequire vector DBMS to havegood scalabilitywith respect to bothdata volumeandcomputing resourceas their corpus grows quickly.In Figure 10 and Figure 11, we test the scalability ofManuwhenchanging the number of query nodes and the size of dataset, respec-tively. The results show that query processing throughput scalesalmost linearly with the number of query nodes and the reciprocalof dataset size. The observation is consistent for different datasets,indexes and similarity functions. This is becauseManuuses seg-ments to distribute search tasks among query nodes. With segmentsize fixed, each query node handles more segments when data vol-ume increases, and fewer segments when the number of querynodes increases. Note that better scalability w.r.t. data volume canbe achieved by configuringManuto use larger segments whendataset size increases. This is because similarity search indexesusually have sub-linear complexity w.r.t. dataset size.Virus scan.Company C is a world leading software security serviceprovider and one of its main service is scanning viruses for smartphones. They have a virus base that continuously collects newviruses and develop specialized algorithms to map virus and userAPK to vector embedding. To conduct a virus scan, they find virusesin their base that have embedding similar to the query APK and thencompare the search results with the APK in more detail. They havetwo requirements for vector DBMS: (1)short delayfor streaming update as new viruses (vectors) are continuously added to theirvirus base and vector search needs to observe the latest viruseswith a short delay; (2)fast index buildingas they frequently adjusttheir embedding algorithm to fix problems, which leads to updateof the entire dataset and requires to rebuild index.In Figure 12, we show the average delay of search requests forManu. Recall thatgrace time(i.e.,ùúè) means that a search requestmust observe updates that happen timeùúèbefore it, and is con-figurable by users. The legends correspond to differenttime tickinterval, with which the loggers write time tick to WAL. The resultsshow that search latency decreases quickly with grace time, andshorter time tick interval results in shorter search latency. This isbecause with longer grace time, search requests can tolerate longerupdate delay and are less likely to wait for updates. When the timetick interval reduces, each segment can confirm that all updateshave been received more quickly, thus the search requests wait fora shorter time. In Figure 13, we report the index building time ofManuwhen changing data volume. The results show that indexbuilding time scales linearly with data volume. This is becauseManubuilds index for each segment and larger data volume leadsto more segments.

6  RELATED WORKVector search algorithms.Vector search algorithms have a longresearch history, and most works focus on efficient approximatesearch on large-scale datasets. Existing algorithms can be roughlyclassified into four categories, i.e.,space partitioning tree(SPT),locality sensitive hashing(LSH),vector quantization(VQ) andprox-imity graph(PG). SPT algorithms divide the space into areas, and use tree structures to quickly narrow down search results to someareas [29,57,63,70,79]. LSH algorithms design hash functions suchthat similar vectors are hashed to the same bucket with high prob-ability, and examples include [34,35,40,42,52,55,56,59,69,89].VQ algorithms compress vectors and accelerate similarity compu-tation by quantizing the vectors using a set of vector codebooks,and well-known VQ algorithms include [22,33,41,44,90]. PG al-gorithms form a graph by connecting a vector with those mostsimilar to it in the dataset, and conduct vector search by graphwalk [32,60,72,88]. Different algorithms have different trade-offs,e.g., LSH is cheap in indexing building but poor in result quality,VQ reduces memory and computation but also harms result quality,PG has high efficiency but requires large memory.Manusupportsa comprehensive set of search algorithms such that users can tradeoff between different factors.Vector databases.Vector data management solutions have gonethrough two stages of development. Solutions in the first stageare libraries (e.g., Facebook Faiss [45], Microsoft SPTAG [15], HN-SWlib [60] and Annoy [1]) and plugins (e.g., ES plugins [6], Postgresplugins [11]) for vector search. They are insufficient for current ap-plications as full-fledged management functionalities are required,e.g., distributed execution for scalability, online data update, andfailure recovery. Two OLAP database systems, AnalyticDB-V [81]and PASE [84] support vector data by adding a table column tostore them but lacks optimizations tailored for vector data.The second stage solutions are full-fledged vector databases suchas Vearch [51], Vespa [18], Weaviate [19], Vald [17], Qdrant [12],Pinecone [10], and our previous effort Milvus [80].6Vearch usesFaiss as the underlying search engine and adopts a three-layer ag-gregation procedure to conduct distributed search. Similarly, Vespadistributes data over nodes for scalability. A modified version of theHNSW algorithm is used to support online updates for vector data,and Vespa also allows attribute filtering during search and learning-based inference on search results (e.g., for re-ranking). Weaviateadopts a GraphQL interface and allows storing objects (e.g., texts,images), properties, and vectors. Users can directly import vec-tors or customize embedding models to map objects to vectors,and Weaviate can retrieve objects based on vector search results.Vald supports horizontal scalability by partitioning a vector datasetinto segments and builds indexes without stopping search services.Qdrant is a single-machine vector search engine with extensivesupport for attribute filtering. It allows filtering with various datatypes and query conditions (e.g., string matching, numerical ranges,geo-locations), and uses a tailored optimizer to determine the fil-tering strategy. Note that Vespa, Weaviate and Vald only supportproximity graph index.We can observe that these vector databases focus on differentfunctionalities, e.g., learning-based inference, embedding genera-tion, object retrieval, and attribute filtering. Thus, we treat evolv-ability as first class priority when designManusuch that new func-tionalities can be easily introduced.Manualso differs from thesevector databases in important perspectives. First, the log backboneofManuprovides time semantics and allows tunable consistency. Second,Manudecomposes system functionalities with fine granu-larity and instantiates them as cloud services for performance andfailure isolation, and thus is more suitable for cloud deployment.Third,Manucomes with more comprehensive optimizations forusability and performance, e.g., support various indexes, hardwaretailored implementations, and GUI tools.Cloud native databases.Many OLAP databases are designedto run on the cloud recently and examples include Redshift [37],BigQuery [61], Snowflake [28] and AnalyticDB [87]. Redshift is adata warehouse system offered as a service on Amazon Web Serviceand adopts ashared-nothingarchitecture. It scales by adding orremoving EC2 instances, and data is redistributed in the granularityof columns. Snowflake uses ashared-dataarchitecture by delegatingdata storage to Amazon S3. Compute nodes are stateless and fetchread-only copies of data for tasks, and thus can be easily scaled. Forefficiency, high-performance local disk is used to cache hot data.Aurora [77] and PolarDB Serverless [50] are two cloud nativeOLTP databases. Aurora uses ashared-diskarchitecture and pro-poses the ‚Äúlog is database" principle by pushing transaction pro-cessing down to the storage engine. It observes that the bottleneckof cloud-based platforms has shifted from computation and storageIO to network IO. Thus, it only persists redo log for transactionprocessing and commits transaction by processing log according toLSN. PolarDB Serverless adopts adisaggregationarchitecture, whichuses high-speed RDMA network to decouple hardware resources(e.g., compute, memory and storage) as resource pools.OurManufollows the general design principles of cloud nativedatabases to decouple the system functionalities at fine granularityfor high elasticity, fast evolution and failure isolation. However, wealso consider the unique design opportunities of vector databases totrade the simple data model and weak consistency requirement forperformance, cost and flexibility. Specifically, complex transactionsare not supported and the log backbone is utilized to support tunableconsistency-performance trade-off. Moreover, vector search, indexbuilding and log archiving tasks are further decoupled as theirworkloads may have significant variations. 
7  CONCLUSIONS AND FUTURE DIRECTIONSIn this paper, we introduce the designs ofManuas a cloud nativevector database. To ensure thatManusuits vector data applications,we set ambitious design goals, which include good evolvability, tun-able consistency, high elasticity, good efficiency, and etc. To meetthese design goals,Manutrades the simple data model of vectorsand weak consistency requirement of applications for performance,costs and flexibility. Specifically,Manuconducts fine-grained de-coupling of the system functionalities for component-wise scalingand evolution, and uses the log backbone to connect the systemcomponents while providing time semantics and simplifying inter-component interaction. We also introduce important features suchas high-level API, GUI tool, hardware optimizations, and complexsearch. We thinkManuis still far from perfect and some of ourfuture directions include:‚Ä¢Multi-way search:Many applications jointly search multipletypes of contents, e.g., vector and primary key, vector and text.The log system ofManuallows to add search engines for other contents (e.g., primary key and text) as co-processors by sub-scribing to the log stream. We will explore how multiple searchengines can interact efficiently and how to flexibly coordinatedifferent search engines to meet application requirements.‚Ä¢Modularized algorithms:We think vector search algorithmscan be distilled into independent components, e.g.,compressionfor memory reduction and efficient computation,indexingforlimiting computation to a small portion of vectors, andbucketingfor grouping similar vectors. Existing vector search algorithmsonly explore some combinations of techniques for different com-ponents. We will provide a unified framework for vector searchsuch that users can flexibly combine different techniques accord-ing to their desired trade-off between cost and performance.‚Ä¢Hierarchical storage aware index:Current vector search indexassumes a single type of storage, e.g., GPU memory, main mem-ory or disk. We will explore indexes that can jointly utilize alldevices on the storage hierarchy. For example, most applicationshave some hot vectors (e.g., popular products in e-commerce)that are frequently accessed by search requests, which can beplaced in fast storage. As a query accesses only a portion ofthe vectors and a node processes many concurrent queries, thestorage swap latency may be hidden by pipelining.‚Ä¢Advanced hardware:NVM [66] costs about one-third of DRAMfor unit capacity but provides comparable read bandwidth andlatency comparable, which makes it a good choice for replacingexpensive DRAM when storing large datasets. RDMA [24,46]significantly reduces the communication latency among nodes,and NVLink [65] directly connects GPUs with much larger band-width than PCIe. By exploiting these fast connections, we willexplore indexes and search algorithms that jointly use multipledevices. We are also working with hardware vendors to applyFPGA and MLU for vector search and index building.‚Ä¢Embedding generation toolbox:For better application level-integration, we plan to incorporate a application-oriented toolboxfor generating embedding vectors. This toolbox would incorpo-rate model fine-tuning in addition to providing a number ofpre-trained models that can be used out-of-the-box, allowing forrapid prototyping.ACKNOWLEDGMENTSManuis a multi-year project open sourced by Zilliz. The devel-opment ofManuinvolves many engineers in its community. Inparticular, we thank Bingyi Sun, Weida Zhu, Yudong Cai, YihuaMo, Xi Ge, Yihao Dai, Jiquan Long, Cai Zhang, Congqi Xia, XuanYang, Binbin Lv, Xiaoyun Liu, Wenxing Zhu, Yufen Zong, Jie Zeng,Shaoyue Chen, Jing Li, Zizhao Chen, Jialian Ji, Min Tian, Yan Wangand all the other contributors in the community for their contri-butions. We also thank Filip Haltmayer for proofreading the paperand valuable suggestions to improve paper quality.
