Gradient compression (GC) is a promising approach to ad- dressing the communication bottleneck in distributed deep learning (DDL). However, it is challenging to find the opti- mal compression strategy for applying GC to DDL because of the intricate interactions among tensors. To fully unleash the benefits of GC, two questions must be addressed: 1) How to express all compression strategies and the corresponding interactions among tensors of any DDL training job? 2) How to quickly select a near-optimal compression strategy? In this paper, we propose ByteComp to answer these questions. It first designs a decision tree abstraction to express all the compression strategies and develops empirical models to timeline tensor computation, communication, and compres- sion to enable ByteComp to derive the intricate interactions among tensors. It then designs a compression decision al- gorithm that analyzes tensor interactions to eliminate and prioritize strategies and optimally offloads compression to CPUs. Experimental evaluations show that ByteComp can improve the training throughput over the start-of-the-art compression-enabled system by up to 77% for representa- tive DDL training jobs. Moreover, the computational time needed to select the compression strategy is measured in milliseconds, and the selected strategy is only a few percent from optimal. 1 Introduction Deep Neural Networks (DNNs) have brought remarkable suc- cess to domains such as computer vision [24, 41, 61, 64] and natural language processing (NLP) [20, 31, 39, 66]. Because today’s training jobs with a single GPU typically take days and even weeks [17, 45], data-parallel distributed deep learn- ing (DDL) has become the norm to accelerate the training with multiple GPUs [3, 27, 34, 35, 59]. However, there exists an exacerbating tension between computation and communication in DDL. The recent innova- tions of hardware accelerators [37, 46] and domain-specific software optimization [15, 16, 76] have dramatically reduced the computation time of DNN training. For example, the single-GPU iteration time of ResNet50 has seen a 22× de- crease in the last seven years [63]. This trend leads to more frequent gradient synchronization in DDL and puts higher pressure on the network. However, it is difficult for GPU cloud network deployments to match this pace; network bandwidth has grown only by roughly 10× in the same pe- riod [40, 46, 49, 57, 79]. The growing concern of communication bottlenecks in DDL has motivated numerous works, such as priority-based scheduling [23, 25, 50], wait-free back-propagation mech- anism [35, 74], and optimized aggregation algorithms [17, 27, 59]. However, even with the latest highly-optimized BytePS [27] which incorporates these state-of-the-art approaches, com- munications for gradient synchronization still account for 42% and 49% of the total training time of GPT2 [51] and BERT-base [20] with 64 NVIDIA V100 GPUs in 8 machines connected by a 100Gbps Ethernet network. Gradient compression (GC) algorithms [5, 6, 29, 36, 58, 62, 70, 71] have a great potential to address the communication bottlenecks in DDL by saving up to 99.9% of the gradient exchange while preserving the training accuracy and conver- gence [26, 62, 72]. However, the training speedups of DDL with GC are only modest because of the costly compression operations. For example, applying GC to the aforementioned GPT2 training only achieves a 1.15× speedup. This moti- vates us to revisit GC from the system perspective to fully unleash its benefits for DDL. Applying GC to a DNN model entails many decisions for each tensor, such as whether to compress, the type of compute resources for compression and the communication schemes for compressed tensors. DDL typically involves both communications inside a machine and across machines. Therefore, another decision is whether to apply GC to intra- or inter-machine communication or both. The compression strategy, i.e., the decisions for all tensors, determines the training throughput of compression-enabled DDL. Unfortunately, it is very challenging to make these deci- sions because of the intricate interactions among tensors. Therefore, the first research question we have to answer to unleash the benefits of GC is how to express all possible compression strategies and the corresponding interactions among tensors for any DDL training job? Because of the extremely large search space, even if all the strategies and the interactions are available, the time to find the optimal one can be prohibitive. Hence, the second research question is how to analyze the interactions among tensors to quickly select a near-optimal compression strategy? In this paper, we propose ByteComp to answer these two questions in order to maximize the benefits of GC. We make the following contributions. • We develop a decision tree abstraction for the compression strategy and empirical models for the time of tensor com- putation, communication, and compression to answer the the first question. The abstraction can express all possible compression options of any tensors regardless of different tensor sizes and GC algorithms. Based on the abstraction, ByteComp can express all compression strategies of any DDL training jobs. The empirical models enable ByteComp to de- rive the timeline of tensor computation, communication, and compression of all tensors in a DNN model, and thus their intricate interactions with any compression strategy. • We propose a compression decision algorithm for quickly selecting a near-optimal compression strategy to answer the second question. ByteComp analyzes the interactions among tensors to eliminate a large number of suboptimal compression strategies. Based on the analysis, ByteComp proposes a prioritization method for applying GC to tensors to maximize the benefits, and considers the overlapping time among tensor computation, communication, and compres- sion to make compression decisions for each tensor. Because of different performance trade-offs of GPUs and CPUs for GC, ByteComp finds a provably optimal solution to offload compression from GPUs to CPUs to minimize the resource contentions with tensor computation. • We implement a fully featured system for ByteComp. We implement both GPUs and CPUs compression libraries. We also implement communication libraries to support different communication schemes in both intra- and inter-machine communications. Experimental evaluations demonstrate that with 64 GPUs, ByteComp can improve the training through- put by up to 269% compared with BytePS. It also outper- forms the state-of-the-art compression-enabled system (i.e., HiPress [9]) by up to 77% across representative DNN training jobs. Moreover, the computational time needed by ByteComp to select the compression strategy is measured in millisec- onds, and the performance difference between the selected strategy and the optimal strategy is only a few percent. 2 Background 2.1 Communication in DDL In data-parallel distributed deep learning (DDL), each GPU has a replica of the DNN model. The training dataset is divided into multiple partitions and each GPU takes one par- tition. Training is performed in multiple iterations. At the beginning of an iteration, each GPU consumes a mini-batch of training data from its own partition. It then independently performs forward propagation and backward propagation to generate gradient tensors, which can be aggregated syn- chronously or asynchronously among GPUs. Synchronous data-parallel DDL, where all GPUs communicate the gradi- ent tensors and wait for the aggregated results prior to the next iteration, is the de facto standard used by DDL frame- works [3, 27, 35, 59]; asynchronous data-parallel DDL, where GPUs do not wait for aggregation to complete, can hurt the model accuracy [13]. We focus on synchronous data-parallel DDL because of its wide adoption. Because DDL typically employs multiple machines and each machine has multiple GPUs, it involves both intra- machine and inter-machine communication. Hierarchical communication (as shown in Figure 1) is widely applied in DDL frameworks [14, 27, 35, 59] because the intra-machine network is usually faster than the inter-machine network. There are three phases for gradient synchronization in hier- archical communication: 1) the gradients are first aggregated among GPUs within one machine; 2) they are then aggre- gated across machines; and 3) the aggregated gradients are communicated within one machine again to ensure that all GPUs have the same synchronized results. Flat communi- cation, i.e., all GPUs join the same collective operation and have only one communication phase, is also supported in some frameworks [35, 59]. 2.2 Computation and Communication Tension Because of the layered structure and a layer-by-layer com- putation pattern in DNN models [10], the wait-free back- propagation mechanism (WFBP) [14, 27, 35, 59, 74] is widely adopted to overlap communication with computation in DDL to reduce the iteration time. However, there still exists an exacerbating tension be- tween computation and communication. The recent advance- ments in ML hardware accelerators [46] and specialized software stacks [15, 55, 76] have significantly improved the single-GPU training speed. For instance, the single-GPU it- eration time of ResNet50 has seen a 22× decrease in the last seven years [63]. Faster training speed leads to more fre- quent gradient synchronization and higher demands on the network. Unfortunately, network upgrades have not kept up with the pace of computation-related advancements. The network bandwidth in GPU clouds has only seen a roughly 10× increase in the same period [40, 46, 49]. This imbal- ance between the fast-growing computing capability and the slower-growing communication bandwidth reduces the chance to overlap communication with computation, and results in poor scalability of DDL. To illustrate, we trained real-world DNN models on BytePS- 0.2.5 [27], a highly-optimized DDL framework, with 64 NVIDIA V100 GPUs (8 GPUs per machine) and a 100Gbps inter- machine Ethernet network. We measure the scaling fac- </span><span class="s2">𝑇𝑛</span><span class="s1"> </span><span class="s2">𝑛𝑇</span><span class="s1"> tor [21, 75], which is defined as throughput of a single device and </span><span class="s2">𝑇𝑛</span><span class="s1"> is the throughput of DDL with </span><span class="s2">𝑛</span><span class="s1"> devices. BytePS only achieves the scaling factors of 0.58 and 0.51 for the training of two representative and popular DNN models, GPT2 and BERT-base, with NVLink 2.0 for GPU-to-GPU interconnection, as shown in Table 1. To put this into context, the training time of BERT-base is about 1200 GPU hours under ideal linear scaling [47], but in practice, it will take 2350 GPUs hours with 64 GPUs due to the communication time caused by gradient synchroniza- tion. Thus, DNN practitioners have to spend nearly twice the amount of money on training because the cost linearly increases with the required GPU hours [8]. When network bandwidth in GPU clouds has not kept pace with the improvements in computation, an alternative is to shrink the communicated traffic volume by applying gradient compression.>
