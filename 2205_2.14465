ByteComp: Revisiting Gradient Compression in Distributed Training

Abstract
Gradient compression (GC) is a promising approach to ad- dressing the communication bottleneck in distributed deep learning (DDL). However, it is challenging to find the opti- mal compression strategy for applying GC to DDL because of the intricate interactions among tensors. To fully unleash the benefits of GC, two questions must be addressed: 1) How to express all compression strategies and the corresponding interactions among tensors of any DDL training job? 2) How to quickly select a near-optimal compression strategy? In this paper, we propose ByteComp to answer these questions. It first designs a decision tree abstraction to express all the compression strategies and develops empirical models to timeline tensor computation, communication, and compres- sion to enable ByteComp to derive the intricate interactions among tensors. It then designs a compression decision al- gorithm that analyzes tensor interactions to eliminate and prioritize strategies and optimally offloads compression to CPUs. Experimental evaluations show that ByteComp can improve the training throughput over the start-of-the-art compression-enabled system by up to 77% for representa- tive DDL training jobs. Moreover, the computational time needed to select the compression strategy is measured in milliseconds, and the selected strategy is only a few percent from optimal.

1 Introduction
Deep Neural Networks (DNNs) have brought remarkable suc- cess to domains such as computer vision [24, 41, 61, 64] and natural language processing (NLP) [20, 31, 39, 66]. Because todayâ€™s training jobs with a single GPU typically take days and even weeks [17, 45], data-parallel distributed deep learn- ing (DDL) has become the norm to accelerate the training with multiple GPUs [3, 27, 34, 35, 59].
However, there exists an exacerbating tension between computation and communication in DDL. The recent innova- tions of hardware accelerators [37, 46] and domain-specific software optimization [15, 16, 76] have dramatically reduced the computation time of DNN training. For example, the single-GPU iteration time of ResNet50 has seen a 22Ã— de- crease in the last seven years [63]. This trend leads to more frequent gradient synchronization in DDL and puts higher pressure on the network. However, it is difficult for GPU cloud network deployments to match this pace; network bandwidth has grown only by roughly 10Ã— in the same pe- riod [40, 46, 49, 57, 79].
The growing concern of communication bottlenecks in DDL has motivated numerous works, such as priority-based scheduling [23, 25, 50], wait-free back-propagation mech- anism [35, 74], and optimized aggregation algorithms [17, 27, 59]. However, even with the latest highly-optimized BytePS [27] which incorporates these state-of-the-art approaches, com- munications for gradient synchronization still account for 42% and 49% of the total training time of GPT2 [51] and BERT-base [20] with 64 NVIDIA V100 GPUs in 8 machines connected by a 100Gbps Ethernet network.
Gradient compression (GC) algorithms [5, 6, 29, 36, 58, 62, 70, 71] have a great potential to address the communication bottlenecks in DDL by saving up to 99.9% of the gradient exchange while preserving the training accuracy and conver- gence [26, 62, 72]. However, the training speedups of DDL with GC are only modest because of the costly compression operations. For example, applying GC to the aforementioned GPT2 training only achieves a 1.15Ã— speedup. This moti- vates us to revisit GC from the system perspective to fully unleash its benefits for DDL.
Applying GC to a DNN model entails many decisions for each tensor, such as whether to compress, the type of compute resources for compression and the communication schemes for compressed tensors. DDL typically involves both communications inside a machine and across machines. Therefore, another decision is whether to apply GC to intra- or inter-machine communication or both. The compression strategy, i.e., the decisions for all tensors, determines the training throughput of compression-enabled DDL.
Unfortunately, it is very challenging to make these deci- sions because of the intricate interactions among tensors. Therefore, the first research question we have to answer to unleash the benefits of GC is how to express all possible compression strategies and the corresponding interactions among tensors for any DDL training job? Because of the extremely large search space, even if all the strategies and the interactions are available, the time to find the optimal one can be prohibitive. Hence, the second research question is how to analyze the interactions among tensors to quickly select a near-optimal compression strategy?
In this paper, we propose ByteComp to answer these two questions in order to maximize the benefits of GC. We make the following contributions.
â€¢ We develop a decision tree abstraction for the compression strategy and empirical models for the time of tensor com- putation, communication, and compression to answer the the first question. The abstraction can express all possible compression options of any tensors regardless of different tensor sizes and GC algorithms. Based on the abstraction, ByteComp can express all compression strategies of any DDL training jobs. The empirical models enable ByteComp to de- rive the timeline of tensor computation, communication, and compression of all tensors in a DNN model, and thus their intricate interactions with any compression strategy.
â€¢ We propose a compression decision algorithm for quickly selecting a near-optimal compression strategy to answer the second question. ByteComp analyzes the interactions among tensors to eliminate a large number of suboptimal compression strategies. Based on the analysis, ByteComp proposes a prioritization method for applying GC to tensors to maximize the benefits, and considers the overlapping time among tensor computation, communication, and compres- sion to make compression decisions for each tensor. Because of different performance trade-offs of GPUs and CPUs for GC, ByteComp finds a provably optimal solution to offload compression from GPUs to CPUs to minimize the resource contentions with tensor computation.
â€¢ We implement a fully featured system for ByteComp. We implement both GPUs and CPUs compression libraries. We also implement communication libraries to support different communication schemes in both intra- and inter-machine communications. Experimental evaluations demonstrate that with 64 GPUs, ByteComp can improve the training through- put by up to 269% compared with BytePS. It also outper- forms the state-of-the-art compression-enabled system (i.e., HiPress [9]) by up to 77% across representative DNN training jobs. Moreover, the computational time needed by ByteComp to select the compression strategy is measured in millisec- onds, and the performance difference between the selected strategy and the optimal strategy is only a few percent.

2 Background
2.1 Communication in DDL
In data-parallel distributed deep learning (DDL), each GPU has a replica of the DNN model. The training dataset is 
divided into multiple partitions and each GPU takes one par- tition. Training is performed in multiple iterations. At the beginning of an iteration, each GPU consumes a mini-batch of training data from its own partition. It then independently performs forward propagation and backward propagation to generate gradient tensors, which can be aggregated syn- chronously or asynchronously among GPUs. Synchronous data-parallel DDL, where all GPUs communicate the gradi- ent tensors and wait for the aggregated results prior to the next iteration, is the de facto standard used by DDL frame- works [3, 27, 35, 59]; asynchronous data-parallel DDL, where GPUs do not wait for aggregation to complete, can hurt the model accuracy [13]. We focus on synchronous data-parallel DDL because of its wide adoption.
Because DDL typically employs multiple machines and each machine has multiple GPUs, it involves both intra- machine and inter-machine communication. Hierarchical communication (as shown in Figure 1) is widely applied in DDL frameworks [14, 27, 35, 59] because the intra-machine network is usually faster than the inter-machine network. There are three phases for gradient synchronization in hier- archical communication: 1) the gradients are first aggregated among GPUs within one machine; 2) they are then aggre- gated across machines; and 3) the aggregated gradients are communicated within one machine again to ensure that all GPUs have the same synchronized results. Flat communi- cation, i.e., all GPUs join the same collective operation and have only one communication phase, is also supported in some frameworks [35, 59].
2.2 Computation and Communication Tension
Because of the layered structure and a layer-by-layer com- putation pattern in DNN models [10], the wait-free back- propagation mechanism (WFBP) [14, 27, 35, 59, 74] is widely adopted to overlap communication with computation in DDL to reduce the iteration time.
However, there still exists an exacerbating tension be- tween computation and communication. The recent advance- ments in ML hardware accelerators [46] and specialized software stacks [15, 55, 76] have significantly improved the single-GPU training speed. For instance, the single-GPU it- eration time of ResNet50 has seen a 22Ã— decrease in the last seven years [63]. Faster training speed leads to more fre- quent gradient synchronization and higher demands on the network. Unfortunately, network upgrades have not kept up with the pace of computation-related advancements. The network bandwidth in GPU clouds has only seen a roughly 10Ã— increase in the same period [40, 46, 49]. This imbal- ance between the fast-growing computing capability and the slower-growing communication bandwidth reduces the chance to overlap communication with computation, and results in poor scalability of DDL.
To illustrate, we trained real-world DNN models on BytePS- 0.2.5 [27], a highly-optimized DDL framework, with 64 NVIDIA V100 GPUs (8 GPUs per machine) and a 100Gbps inter-
machine Ethernet network. We measure the scaling fac-
ð‘‡ð‘›
ð‘›ð‘‡
tor [21, 75], which is defined as
throughput of a single device and ð‘‡ð‘› is the throughput of DDL with ð‘› devices. BytePS only achieves the scaling factors of 0.58 and 0.51 for the training of two representative and popular DNN models, GPT2 and BERT-base, with NVLink 2.0 for GPU-to-GPU interconnection, as shown in Table 1. To put this into context, the training time of BERT-base is about 1200 GPU hours under ideal linear scaling [47], but in practice, it will take 2350 GPUs hours with 64 GPUs due to the communication time caused by gradient synchroniza- tion. Thus, DNN practitioners have to spend nearly twice the amount of money on training because the cost linearly increases with the required GPU hours [8].
When network bandwidth in GPU clouds has not kept pace with the improvements in computation, an alternative is to shrink the communicated traffic volume by applying gradient compression.
2.3 Gradient Compression
Many gradient compression (GC) algorithms have been pro- posed in the machine learning community. Sparsification and Quantization are the two main types of GC algorithms. Sparsification selects a subset of the original stochastic gra- dients for synchronization [5, 36, 62] and it can save up to 99.9% of the gradient exchange while maintaining model accuracy [36]. Quantization decreases the precision of gra- dients; gradients in single-precision floating-point format (FP32) are mapped to fewer bits, such as 8 bits [19], 2 bits [71], and even 1 bit [11, 29, 58] to reduce the communicated traf- fic volume by up to 96.9%. Such compression algorithms have been theoretically proven and/or empirically validated to preserve the convergence of model training and impose negligible impact on model accuracy when combined with error-feedback mechanisms [26, 36, 58, 62, 72]. The industry is adopting GC because of its great potential to alleviate the communication bottleneck in DDL. The efforts from Meta, AWS, and ByteDance to bring GC to mainstream DNN sys- tems have begun recently [7, 44, 78]. However, the scalability improvement of DDL via GC has been still poor.
3 Challenges of Applying GC to DDL
We first define some key terms.
â€¢ Tensor computation is the computation of a tensor dur- ing backward propagation.
â€¢ Communication time is the wall-clock time for commu- nication. It is denoted as ðœð‘ð‘œð‘šð‘š .
â€¢ Communication overhead is the communication time that cannot overlap with tensor computation of any tensors. It is denoted as ð‘œð‘ð‘œð‘šð‘š .
â€¢ Compression time is the wall-clock time to perform compression and decompression operations on devices, e.g., GPUs or CPUs. It is denoted as ðœð‘ð‘œð‘šð‘
â€¢ Compression overhead is the compression time that can- not overlap with either tensor computation or communica- tion of any tensors. It is denoted as ð‘œð‘ð‘œð‘šð‘ .
Although GC can reduce ðœ , its compression over- ð‘ð‘œð‘šð‘š
heads can dramatically dilute the benefits gained from the reduced communication time. To demonstrate this, we apply a popular sparsification algorithm, DGC [36], to the afore- mentioned GPT2 training and a representative 1-bit quan- tization algorithm, EFSignSGD [29], to BERT-base training. The compression rate of DGC is 1%, i.e., only 1% of gra- dients are exchanged during synchronization. Tensors are compressed with GPUs [9] or CPUs [78] in separate exper- iments. As shown in Table 1, GC only achieves up to 20% training speedup, which is on par with the findings in prior works [4, 9, 73]. In fact, GC can harm performance in some situations. To illustrate, we apply DGC with 1% compres- sion rate to the training of LSTM [41] on 64 V100 GPUs with PCIe 3.0 x16 as the intra-machine network and 25Gbps
1
inter-machineEthernet. AslistedinTable1,GCslowsdown
training by up to 9%.
In the following, we will explain the root reasons why it
is challenging to obtain large benefits from GC for DDL.
3.1 Root Reasons of the Challenges
The choice of compression strategies determines the iteration time of compression-enabled DDL. Figure 2 is an example that shows the timelines of tensor computation, communi- cation, and compression of DDL with different compression strategies. Figures 2(a) is the baseline without GC and it il- lustrates the tensor computation time (blue boxes) and com- munication time (green boxes) of all tensors, i.e., T0, T1, and T2. Figure 2(b) compresses T2 with GPUs and it reduces the iteration time. Figures 2(c) and (d) compress the three tensors with GPUs and CPUs, respectively, but unfortunately, they both harm the performance of DDL. Figures 2(e) shows the optimal compression strategy with ByteComp.
It is challenging to find the optimal compression strategy. Applying GC to DDL is essentially to reduce the communi- cation overheads at the cost of the compression overheads. The optimal compression strategy maximizes the difference between the reduced communication overheads and the in- curred compression overheads. There are three root reasons for the challenges.
Reason #1. It is hard to quantify the communication and compression overheads because of the intricate interactions among tensors.
Communication may or may not overlap with ten-
sor computation. The overlapping time of different tensors
can vary. For example, in Figure 2(a), T â€™s ð‘œ is zero be- 0 ð‘ð‘œð‘šð‘š
then compresses the aggregated tensor and obtains ð‘‡ð‘– ð‘— for the second communication op.
cause its communication is fully overlapped with tensor computation, but T â€™s ð‘œ is its communication time be-
2 ð‘ð‘œð‘šð‘š
cause it has no overlap with tensor computation. Moreover,
the overlapping time of one tensor can vary under different compressionstrategies.Forexample,inFigure2(a),T â€™scom-
munication partially overlaps with T â€™s tensor computation.
However, in Figure 2(c), after compression, T â€™s communica-
tion can completely overlap with T â€™s tensor computation.
Furthermore, in Figure 2(d), T â€™s communication has no
overlap with the computation of other tensors. Hence, it is difficult to quantify the communication overhead of each tensor.
Compression may or may not overlap with tensor
computation and communication. How much ðœð‘ð‘œð‘šð‘ can
be overlapped highly depends on the strategy. For instance, in Figure 2(b), T â€™s GPU compression fully overlaps with T â€™s communication. In Figure 2(d), T â€™s CPU compression par-
tially overlaps with T â€™s tensor computation. In Figure 2(c),
the three GPU compressions are fully exposed. Hence, it is difficult to quantify the compression overhead.
Only considering ðœð‘ð‘œð‘šð‘š and ðœð‘ð‘œð‘šð‘ for the decision of compression strategies can harm the performance. Fig- ure 2(c) maximizes the difference between the reduced com- munication time and the compression time by compressing the three tensors. However, because GPU compression com- petes for compute resources with tensor computation, it de- lays training and prolongs the iteration time instead. Hence, we must consider ð‘œð‘ð‘œð‘šð‘š and ð‘œð‘ð‘œð‘šð‘ to determine compression strategies for compression-enabled DDL.
Reason #2. It is hard to choose the right communication schemes for compressed tensors because of Reason #1.
There are two types of communication schemes for compressed tensors: indivisible schemes and divisible schemes. We first consider the case that there are ð‘ machines in DDL and each machine has a single GPU. An indivisible scheme has only one communication operation, as shown in Fig- ure 3. Once a tensor is compressed, each node (e.g., GPU or CPU) broadcasts its compressed tensor to other nodes. After communication, each node decompresses these com- pressed tensors and aggregates them. In contrast, a divisible scheme has two communication operations, as shown in Figure 4. Tensors are first compressed and partitioned into ð‘› parts, where 1 â‰¤ ð‘› â‰¤ ð‘ . The ð‘—ð‘¡h node receives the ð‘—ð‘¡h part from other nodes. It then performs decompression, aggre-
2 gation,andthesecondcompressionoperation. Afterthat,
it broadcasts the compressed tensor to other nodes. After communication, each node decompresses these compressed tensors and aggregates them.
It is hard to decide between indivisible and divisible
schemes for GC. Compared to indivisible schemes, divisible
schemes have lower communication time and higher com-
pression time due to the two compression and decompression
operations. As shown in Figures 5(a), GC with a divisible
scheme outperforms GC with an indivisible scheme. How-
ever, in Figure 5(b), T â€™s communication overlaps with T â€™s
01
tensor computation and an indivisible scheme outperforms a divisible scheme for GC. Thus, the decision of communica- tion schemes depends on the interactions among tensors.
Reason #3. It is hard to determine whether to apply GC to intra- or inter-machine communication or both to alleviate communication bottleneck because of Reasons #1 and #2.
DDL can involve both intra- and inter-machine com- munications. We now consider the case that there are ð‘ machines and each machine has ð‘˜ GPU, where ð‘˜ > 1, as shown in Figure 1. It has intra- and inter-machine communi- cations, and both can become the performance bottleneck.
Whether to apply GC to intra- or inter-machine com-
munication or both depends on the interactions among
tensors. If a tensor is only compressed for inter-machine
communication, intra-machine communication can still be
a performance issue. Figure 5(c) shows that applying GC to
intra-machine communication can further reduce the itera-
tion time. However, if T1 has a longer computation time, it
can overlap more time with T â€™s communication, as shown
in Figure 5(d). In this case, applying GC to both intra- and inter-machine communications leads to worse performance than applying it to inter-machine communication alone.
This decision also depends on the chosen commu- nication schemes. Because both intra- and inter-machine communications need to choose from indivisible or divisi- ble schemes, the difficulties to determine the right schemes make the decision of the compression choices even harder.
